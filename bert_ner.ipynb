{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T05:42:52.153580Z",
     "iopub.status.busy": "2024-11-19T05:42:52.152872Z",
     "iopub.status.idle": "2024-11-19T05:42:52.161681Z",
     "shell.execute_reply": "2024-11-19T05:42:52.160092Z",
     "shell.execute_reply.started": "2024-11-19T05:42:52.153518Z"
    }
   },
   "source": [
    "参考：https://github.com/vaibhavdangar09/NER-WITH-BERT/blob/main/NER_WITH_BERT.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "dbname = 'other' \n",
    "engine = create_engine(f'postgresql://postgres:123@localhost:5432/{dbname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sql = \"\"\"\n",
    "    SELECT text FROM mfa_cn\n",
    "    UNION\n",
    "    SELECT title as text FROM mfa_usa;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(text(sql), con=engine)\n",
    "except:\n",
    "    df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 将数据写入 txt\n",
    "- 利用标注工具标注实体\n",
    "- 使用[MarkStudio](https://github.com/cuiwang/MarkStudio)进行标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功写入 data/ner_label_in.txt 文件！\n"
     ]
    }
   ],
   "source": [
    "# 将每一行数据写入txt文件\n",
    "txt_file = 'data/ner_label_in.txt'\n",
    "with open(txt_file, 'w', encoding='utf-8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write(row['text'] + '\\n')  #\n",
    "\n",
    "print(f\"数据已成功写入 {txt_file} 文件！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.381007Z",
     "iopub.status.idle": "2024-11-19T09:06:29.390589Z",
     "shell.execute_reply": "2024-11-19T09:06:29.389496Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.380979Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128  # 每条数据最大长度\n",
    "BATCH_SIZE = 8  # 批处理大小\n",
    "NUM_LABELS = 3  # NER标记数量 (e.g., B-LOC, I-LOC, O, etc.)\n",
    "MODEL_NAME = 'bert-base-chinese'  # 模型名称\n",
    "# MODEL_PATH = 'model/'  # 模型路径\n",
    "MODEL_PATH = r'E:/JupyterLab//LLM//Large-Model//bert//'  # 模型路径\n",
    "LABEL_DATA_PATH = 'data/label_data.json'  # 标注数据路径\n",
    "OUT_DIR = 'model/'  # 输出路径\n",
    "LOG_DIR = 'log/'  # 日志路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.392557Z",
     "iopub.status.busy": "2024-11-19T09:06:29.392199Z",
     "iopub.status.idle": "2024-11-19T09:06:29.397763Z",
     "shell.execute_reply": "2024-11-19T09:06:29.396547Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.392527Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = ['O','B-PLACE','I-PLACE']  # 根据你自己的标记集合进行修改\n",
    "id2label = {\n",
    "    i: label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: i for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.399794Z",
     "iopub.status.busy": "2024-11-19T09:06:29.399236Z",
     "iopub.status.idle": "2024-11-19T09:06:29.411228Z",
     "shell.execute_reply": "2024-11-19T09:06:29.410132Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.399746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O', 1: 'B-PLACE', 2: 'I-PLACE'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.427442Z",
     "iopub.status.busy": "2024-11-19T09:06:29.427117Z",
     "iopub.status.idle": "2024-11-19T09:06:29.432960Z",
     "shell.execute_reply": "2024-11-19T09:06:29.431897Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.427414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B-PLACE': 1, 'I-PLACE': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据处理\n",
    "\n",
    "- MarkStudio输出的格式为JOSN文件\n",
    "- 我们预取的数据格式为：\n",
    "\n",
    "    ```python\n",
    "    text: ['外交部中阿合作论坛事务大使李琛访问卡塔尔']\n",
    "    label: [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2]\n",
    "    ```\n",
    "- 我们使用 [BIO](https://blog.csdn.net/HappyRocking/article/details/79716212) 法则标注数据\n",
    "- 0,1,2分别表示非实体，实体开始，实体中间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.436306Z",
     "iopub.status.busy": "2024-11-19T09:06:29.435974Z",
     "iopub.status.idle": "2024-11-19T09:06:30.434850Z",
     "shell.execute_reply": "2024-11-19T09:06:30.434141Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.436278Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.438930Z",
     "iopub.status.busy": "2024-11-19T09:06:30.438733Z",
     "iopub.status.idle": "2024-11-19T09:06:30.449944Z",
     "shell.execute_reply": "2024-11-19T09:06:30.449283Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.438909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: ['拜登总统对国会两院联席会议发表讲话', '中国政府中东问题特使翟隽出席金砖国家中东事务副外长/特使磋商']\n",
      "Labels: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 来自标注好的JSON文件\n",
    "with open(LABEL_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for entry in data:\n",
    "    text = entry['content']\n",
    "    label_sequence = ['O'] * len(text)  # 初始化所有字符的标签为 'O'\n",
    "\n",
    "    for tag in entry['tags']:\n",
    "        if tag['name'] == 'PLACE':\n",
    "            start = tag['start']\n",
    "            end = tag['end']\n",
    "\n",
    "            # 将开始位置标记为 'B-PLACE'\n",
    "            label_sequence[start] = 'B-PLACE'\n",
    "\n",
    "            # 将后续位置标记为 'I-PLACE'\n",
    "            for i in range(start + 1, end):\n",
    "                label_sequence[i] = 'I-PLACE'\n",
    "\n",
    "    # 将标签转换为标签索引\n",
    "    label_indices = [label2id[label] for label in label_sequence]\n",
    "\n",
    "    texts.append(text)\n",
    "    labels.append(label_indices)\n",
    "\n",
    "# 检查转换后的格式\n",
    "print(\"Texts:\", texts[-2:])\n",
    "print(\"Labels:\", labels[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 还需要对数据进行处理\n",
    "- 首先是进行数据集划分\n",
    "- 然后构建为以下字典形式：\n",
    "```python\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'tokens', 'ner_tags'],\n",
    "        num_rows: 305\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['id', 'tokens', 'ner_tags'],\n",
    "        num_rows: 38\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'tokens', 'ner_tags'],\n",
    "        num_rows: 39\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.450709Z",
     "iopub.status.busy": "2024-11-19T09:06:30.450526Z",
     "iopub.status.idle": "2024-11-19T09:06:30.455718Z",
     "shell.execute_reply": "2024-11-19T09:06:30.455075Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.450690Z"
    }
   },
   "outputs": [],
   "source": [
    "# 划分数据集--训练测试和验证\n",
    "texts_train, texts_temp, labels_train, labels_temp = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "texts_val, texts_test, labels_val, labels_test = train_test_split(\n",
    "    texts_temp, labels_temp, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.456570Z",
     "iopub.status.busy": "2024-11-19T09:06:30.456385Z",
     "iopub.status.idle": "2024-11-19T09:06:30.461436Z",
     "shell.execute_reply": "2024-11-19T09:06:30.460887Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.456551Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构造字典形式的数据\n",
    "def create_dataset(texts, labels):\n",
    "    ids = list(range(len(texts)))\n",
    "    tokens_list = [list(text) for text in texts]\n",
    "    return {'id': ids, 'tokens': tokens_list, 'ner_tags': labels}\n",
    "\n",
    "train_data = create_dataset(texts_train, labels_train)\n",
    "val_data = create_dataset(texts_val, labels_val)\n",
    "test_data = create_dataset(texts_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.490090Z",
     "iopub.status.busy": "2024-11-19T09:06:30.489798Z",
     "iopub.status.idle": "2024-11-19T09:06:30.511792Z",
     "shell.execute_reply": "2024-11-19T09:06:30.511245Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.490068Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建 Dataset 和 DatasetDict\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "ner_data = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.687147Z",
     "iopub.status.busy": "2024-11-19T09:06:30.686411Z",
     "iopub.status.idle": "2024-11-19T09:06:30.692636Z",
     "shell.execute_reply": "2024-11-19T09:06:30.691501Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.687123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 305\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 39\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.880319Z",
     "iopub.status.busy": "2024-11-19T09:06:30.879365Z",
     "iopub.status.idle": "2024-11-19T09:06:30.889726Z",
     "shell.execute_reply": "2024-11-19T09:06:30.888593Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.880267Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'tokens': ['美',\n",
       "  '国',\n",
       "  '宣',\n",
       "  '布',\n",
       "  '向',\n",
       "  '加',\n",
       "  '沙',\n",
       "  '及',\n",
       "  '该',\n",
       "  '地',\n",
       "  '区',\n",
       "  '的',\n",
       "  '巴',\n",
       "  '勒',\n",
       "  '斯',\n",
       "  '坦',\n",
       "  '平',\n",
       "  '民',\n",
       "  '提',\n",
       "  '供',\n",
       "  '更',\n",
       "  '多',\n",
       "  '人',\n",
       "  '道',\n",
       "  '主',\n",
       "  '义',\n",
       "  '援',\n",
       "  '助'],\n",
       " 'ner_tags': [1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:31.616927Z",
     "iopub.status.busy": "2024-11-19T09:06:31.615885Z",
     "iopub.status.idle": "2024-11-19T09:06:33.096302Z",
     "shell.execute_reply": "2024-11-19T09:06:33.095337Z",
     "shell.execute_reply.started": "2024-11-19T09:06:31.616874Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.098043Z",
     "iopub.status.busy": "2024-11-19T09:06:33.097738Z",
     "iopub.status.idle": "2024-11-19T09:06:33.147647Z",
     "shell.execute_reply": "2024-11-19T09:06:33.146805Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.098022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='E:/JupyterLab//LLM//Large-Model//bert//bert-base-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH+MODEL_NAME)  # 自己下载的中文 BERT 模型\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.148810Z",
     "iopub.status.busy": "2024-11-19T09:06:33.148521Z",
     "iopub.status.idle": "2024-11-19T09:06:33.154558Z",
     "shell.execute_reply": "2024-11-19T09:06:33.153758Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.148790Z"
    }
   },
   "outputs": [],
   "source": [
    "example_text = ner_data['train'][0]\n",
    "tokenized_input = tokenizer(example_text['tokens'],is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "word_ids = tokenized_input.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.156157Z",
     "iopub.status.busy": "2024-11-19T09:06:33.155847Z",
     "iopub.status.idle": "2024-11-19T09:06:33.160933Z",
     "shell.execute_reply": "2024-11-19T09:06:33.160116Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.156136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 5401, 1744, 2146, 2357, 1403, 1217, 3763, 1350, 6421, 1765, 1277, 4638, 2349, 1239, 3172, 1788, 2398, 3696, 2990, 897, 3291, 1914, 782, 6887, 712, 721, 3001, 1221, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "['[CLS]', '美', '国', '宣', '布', '向', '加', '沙', '及', '该', '地', '区', '的', '巴', '勒', '斯', '坦', '平', '民', '提', '供', '更', '多', '人', '道', '主', '义', '援', '助', '[SEP]']\n",
      "\n",
      "\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input)\n",
    "print(\"\\n\")\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.161837Z",
     "iopub.status.busy": "2024-11-19T09:06:33.161653Z",
     "iopub.status.idle": "2024-11-19T09:06:33.166600Z",
     "shell.execute_reply": "2024-11-19T09:06:33.165776Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.161817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the tokens is : 30\n",
      "Length of the ner tags is: 28\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the tokens is : {len(tokens)}')\n",
    "print(f'Length of the ner tags is: {len(ner_data[\"train\"][0][\"ner_tags\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在这里，文字标记和token的尺寸是不同的，所以为了使代币和文字标记的尺寸相同，我们在文字标记的第一个和最后一个位置加上 -100。\n",
    "- 在训练过程中，BERT 模型避开了 -100。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.167522Z",
     "iopub.status.busy": "2024-11-19T09:06:33.167337Z",
     "iopub.status.idle": "2024-11-19T09:06:33.173289Z",
     "shell.execute_reply": "2024-11-19T09:06:33.172490Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.167503Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        # Special tokens like `` and `<\\s>` are originally mapped to None\n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # set –100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                # mask the subword representations after the first subword\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.641287Z",
     "iopub.status.busy": "2024-11-19T09:06:33.640385Z",
     "iopub.status.idle": "2024-11-19T09:06:33.648709Z",
     "shell.execute_reply": "2024-11-19T09:06:33.647536Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.641234Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1744, 2157, 2128, 1059, 7560, 7309, 3345, 1046, 185, 3763, 1164, 3152, 1068, 754, 915, 5384, 3172, 1415, 1104, 5468, 1394, 1744, 2128, 4415, 833, 517, 1912, 2231, 4958, 7313, 3340, 5276, 518, 1104, 6379, 4638, 1898, 3209, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = tokenize_and_align_labels(ner_data['train'][1:2])\n",
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.831270Z",
     "iopub.status.busy": "2024-11-19T09:06:33.830355Z",
     "iopub.status.idle": "2024-11-19T09:06:33.838701Z",
     "shell.execute_reply": "2024-11-19T09:06:33.837478Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.831217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "国_______________________________________ 0\n",
      "家_______________________________________ 0\n",
      "安_______________________________________ 0\n",
      "全_______________________________________ 0\n",
      "顾_______________________________________ 0\n",
      "问_______________________________________ 0\n",
      "杰_______________________________________ 0\n",
      "克_______________________________________ 0\n",
      "·_______________________________________ 0\n",
      "沙_______________________________________ 0\n",
      "利_______________________________________ 0\n",
      "文_______________________________________ 0\n",
      "关_______________________________________ 0\n",
      "于_______________________________________ 0\n",
      "俄_______________________________________ 1\n",
      "罗_______________________________________ 2\n",
      "斯_______________________________________ 2\n",
      "否_______________________________________ 0\n",
      "决_______________________________________ 0\n",
      "联_______________________________________ 0\n",
      "合_______________________________________ 0\n",
      "国_______________________________________ 0\n",
      "安_______________________________________ 0\n",
      "理_______________________________________ 0\n",
      "会_______________________________________ 0\n",
      "《_______________________________________ 0\n",
      "外_______________________________________ 0\n",
      "层_______________________________________ 0\n",
      "空_______________________________________ 0\n",
      "间_______________________________________ 0\n",
      "条_______________________________________ 0\n",
      "约_______________________________________ 0\n",
      "》_______________________________________ 0\n",
      "决_______________________________________ 0\n",
      "议_______________________________________ 0\n",
      "的_______________________________________ 0\n",
      "声_______________________________________ 0\n",
      "明_______________________________________ 0\n",
      "[SEP]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(example_text[\"input_ids\"][0]), example_text[\"labels\"][0]):\n",
    "    print(f\"{token:_<40} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:34.008303Z",
     "iopub.status.busy": "2024-11-19T09:06:34.008033Z",
     "iopub.status.idle": "2024-11-19T09:06:34.164409Z",
     "shell.execute_reply": "2024-11-19T09:06:34.163382Z",
     "shell.execute_reply.started": "2024-11-19T09:06:34.008282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7debbb3f23e0487ca9ca92839b94146a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2f8914e7044d6d95cd9496d9c1fe4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a8a9adbc314d6e8eb248013c7c01d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 应用于整个数据\n",
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:34.207791Z",
     "iopub.status.busy": "2024-11-19T09:06:34.207059Z",
     "iopub.status.idle": "2024-11-19T09:06:34.218545Z",
     "shell.execute_reply": "2024-11-19T09:06:34.217368Z",
     "shell.execute_reply.started": "2024-11-19T09:06:34.207753Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'tokens': ['美',\n",
       "  '国',\n",
       "  '宣',\n",
       "  '布',\n",
       "  '向',\n",
       "  '加',\n",
       "  '沙',\n",
       "  '及',\n",
       "  '该',\n",
       "  '地',\n",
       "  '区',\n",
       "  '的',\n",
       "  '巴',\n",
       "  '勒',\n",
       "  '斯',\n",
       "  '坦',\n",
       "  '平',\n",
       "  '民',\n",
       "  '提',\n",
       "  '供',\n",
       "  '更',\n",
       "  '多',\n",
       "  '人',\n",
       "  '道',\n",
       "  '主',\n",
       "  '义',\n",
       "  '援',\n",
       "  '助'],\n",
       " 'ner_tags': [1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids': [101,\n",
       "  5401,\n",
       "  1744,\n",
       "  2146,\n",
       "  2357,\n",
       "  1403,\n",
       "  1217,\n",
       "  3763,\n",
       "  1350,\n",
       "  6421,\n",
       "  1765,\n",
       "  1277,\n",
       "  4638,\n",
       "  2349,\n",
       "  1239,\n",
       "  3172,\n",
       "  1788,\n",
       "  2398,\n",
       "  3696,\n",
       "  2990,\n",
       "  897,\n",
       "  3291,\n",
       "  1914,\n",
       "  782,\n",
       "  6887,\n",
       "  712,\n",
       "  721,\n",
       "  3001,\n",
       "  1221,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:34.865674Z",
     "iopub.status.busy": "2024-11-19T09:06:34.864681Z",
     "iopub.status.idle": "2024-11-19T09:06:35.741734Z",
     "shell.execute_reply": "2024-11-19T09:06:35.740789Z",
     "shell.execute_reply.started": "2024-11-19T09:06:34.865620Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:35.751682Z",
     "iopub.status.busy": "2024-11-19T09:06:35.751472Z",
     "iopub.status.idle": "2024-11-19T09:06:36.359175Z",
     "shell.execute_reply": "2024-11-19T09:06:36.357967Z",
     "shell.execute_reply.started": "2024-11-19T09:06:35.751661Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at E:/JupyterLab//LLM//Large-Model//bert//bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH+MODEL_NAME, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:36.361453Z",
     "iopub.status.busy": "2024-11-19T09:06:36.361067Z",
     "iopub.status.idle": "2024-11-19T09:06:36.389382Z",
     "shell.execute_reply": "2024-11-19T09:06:36.388497Z",
     "shell.execute_reply.started": "2024-11-19T09:06:36.361430Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:36.619080Z",
     "iopub.status.busy": "2024-11-19T09:06:36.618750Z",
     "iopub.status.idle": "2024-11-19T09:06:36.628825Z",
     "shell.execute_reply": "2024-11-19T09:06:36.627671Z",
     "shell.execute_reply.started": "2024-11-19T09:06:36.619058Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_ner_metrics(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    自定义评估函数，输入为二维列表，输出为各指标\n",
    "    \"\"\"\n",
    "    assert len(true_labels) == len(pred_labels), \"true_labels 和 pred_labels 的长度必须一致\"\n",
    "    \n",
    "    # 初始化统计变量\n",
    "    total_true = 0  # 总的真实实体数\n",
    "    total_pred = 0  # 总的预测实体数\n",
    "    total_correct = 0  # 预测正确的实体数\n",
    "    total_tokens = 0  # 总的标注的token数\n",
    "    correct_tokens = 0  # 预测正确的token数\n",
    "    \n",
    "    # 遍历每个序列\n",
    "    for true_seq, pred_seq in zip(true_labels, pred_labels):\n",
    "        assert len(true_seq) == len(pred_seq), \"每个序列的长度必须一致\"\n",
    "        \n",
    "        for true, pred in zip(true_seq, pred_seq):\n",
    "            # 统计 token-level 准确性\n",
    "            total_tokens += 1\n",
    "            if true == pred:\n",
    "                correct_tokens += 1\n",
    "            \n",
    "            # 如果是实体标签，更新统计\n",
    "            if true != \"O\":  # 真实标签为实体\n",
    "                total_true += 1\n",
    "                if true == pred:  # 预测正确的实体\n",
    "                    total_correct += 1\n",
    "            \n",
    "            if pred != \"O\":  # 预测标签为实体\n",
    "                total_pred += 1\n",
    "    \n",
    "    # 计算指标\n",
    "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "    precision = total_correct / total_pred if total_pred > 0 else 0.0\n",
    "    recall = total_correct / total_true if total_true > 0 else 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits, labels = pred\n",
    "    pred_logits = pred_logits.argmax(-1)\n",
    "    # 取去除 padding 的部分\n",
    "    predictions = [\n",
    "        [id2label[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(pred_logits, labels)\n",
    "   ]\n",
    "    result = calculate_ner_metrics(\n",
    "        true_labels,\n",
    "        predictions\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:37.497242Z",
     "iopub.status.busy": "2024-11-19T09:06:37.496128Z",
     "iopub.status.idle": "2024-11-19T09:06:37.614202Z",
     "shell.execute_reply": "2024-11-19T09:06:37.612987Z",
     "shell.execute_reply.started": "2024-11-19T09:06:37.497186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 重写 Trainer 类\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        if self.optimizer is None:\n",
    "            # 获取模型参数\n",
    "            decay_parameters = [\n",
    "                p for n, p in self.model.named_parameters() if n.endswith(\"weight\")\n",
    "            ]\n",
    "            no_decay_parameters = [\n",
    "                p for n, p in self.model.named_parameters() if n.endswith(\"bias\")\n",
    "            ]\n",
    "            # 将参数分组\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\"params\": decay_parameters, \"weight_decay\": self.args.weight_decay},\n",
    "                {\"params\": no_decay_parameters, \"weight_decay\": 0.0},\n",
    "            ]\n",
    "            # 使用 AdamW 作为优化器\n",
    "            self.optimizer = AdamW(\n",
    "                optimizer_grouped_parameters, lr=self.args.learning_rate\n",
    "            )\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "# 创建训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=LOG_DIR,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:38.072437Z",
     "iopub.status.busy": "2024-11-19T09:06:38.071443Z",
     "iopub.status.idle": "2024-11-19T09:06:38.077693Z",
     "shell.execute_reply": "2024-11-19T09:06:38.076486Z",
     "shell.execute_reply.started": "2024-11-19T09:06:38.072383Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据收集器，用于将数据转换为模型可接受的格式\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:38.973147Z",
     "iopub.status.busy": "2024-11-19T09:06:38.972111Z",
     "iopub.status.idle": "2024-11-19T09:06:39.266938Z",
     "shell.execute_reply": "2024-11-19T09:06:39.265972Z",
     "shell.execute_reply.started": "2024-11-19T09:06:38.973093Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义 Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # 替换为你的模型\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:56.918883Z",
     "iopub.status.busy": "2024-11-19T09:06:56.918387Z",
     "iopub.status.idle": "2024-11-19T09:06:56.927024Z",
     "shell.execute_reply": "2024-11-19T09:06:56.925775Z",
     "shell.execute_reply.started": "2024-11-19T09:06:56.918859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:59.405520Z",
     "iopub.status.busy": "2024-11-19T09:06:59.404843Z",
     "iopub.status.idle": "2024-11-19T09:07:01.112018Z",
     "shell.execute_reply": "2024-11-19T09:07:01.110809Z",
     "shell.execute_reply.started": "2024-11-19T09:06:59.405466Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e44f4993c24335b7d0067a1c22621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38517987ba6429f8c394cde74d83855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.029824865981936455, 'eval_accuracy': 0.993993993993994, 'eval_precision': 0.9854368932038835, 'eval_recall': 0.9854368932038835, 'eval_f1_score': 0.9854368932038835, 'eval_runtime': 0.1217, 'eval_samples_per_second': 312.205, 'eval_steps_per_second': 41.08, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88ec27d9e9d4362b7a74b1cb5f9c124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01185962837189436, 'eval_accuracy': 0.998998998998999, 'eval_precision': 0.9951690821256038, 'eval_recall': 1.0, 'eval_f1_score': 0.9975786924939467, 'eval_runtime': 0.1151, 'eval_samples_per_second': 330.147, 'eval_steps_per_second': 43.44, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1074cf9ca34fc8bf41a05009e5d943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012030001729726791, 'eval_accuracy': 0.995995995995996, 'eval_precision': 0.9950980392156863, 'eval_recall': 0.9854368932038835, 'eval_f1_score': 0.9902439024390244, 'eval_runtime': 0.1166, 'eval_samples_per_second': 325.959, 'eval_steps_per_second': 42.889, 'epoch': 3.0}\n",
      "{'train_runtime': 19.4364, 'train_samples_per_second': 47.077, 'train_steps_per_second': 6.02, 'train_loss': 0.10041725941193409, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=117, training_loss=0.10041725941193409, metrics={'train_runtime': 19.4364, 'train_samples_per_second': 47.077, 'train_steps_per_second': 6.02, 'total_flos': 19394825045526.0, 'train_loss': 0.10041725941193409, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练 model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T08:17:45.058370Z",
     "iopub.status.busy": "2024-11-19T08:17:45.057797Z",
     "iopub.status.idle": "2024-11-19T08:17:45.063345Z",
     "shell.execute_reply": "2024-11-19T08:17:45.062571Z",
     "shell.execute_reply.started": "2024-11-19T08:17:45.058349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/checkpoint-78'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ckpt_path = trainer.state.best_model_checkpoint\n",
    "best_ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T08:18:35.309711Z",
     "iopub.status.busy": "2024-11-19T08:18:35.309055Z",
     "iopub.status.idle": "2024-11-19T08:18:35.520661Z",
     "shell.execute_reply": "2024-11-19T08:18:35.519495Z",
     "shell.execute_reply.started": "2024-11-19T08:18:35.309656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82354823ef040a19d9cb9be004b4d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0154577000066638,\n",
       " 'eval_accuracy': 0.9952516619183286,\n",
       " 'eval_precision': 0.9822485207100592,\n",
       " 'eval_recall': 0.9880952380952381,\n",
       " 'eval_f1_score': 0.9851632047477745,\n",
       " 'eval_runtime': 0.1442,\n",
       " 'eval_samples_per_second': 270.544,\n",
       " 'eval_steps_per_second': 34.685,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T08:27:10.144699Z",
     "iopub.status.busy": "2024-11-19T08:27:10.144123Z",
     "iopub.status.idle": "2024-11-19T08:27:10.156436Z",
     "shell.execute_reply": "2024-11-19T08:27:10.155045Z",
     "shell.execute_reply.started": "2024-11-19T08:27:10.144644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'今天，美利坚合众国国防部发言人乔治表示中华人民共和国的歼20战机很优秀。'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试文本\n",
    "input_text = \"今天，美利坚合众国国防部发言人乔治表示中华人民共和国的歼20战机很优秀。\"\n",
    "encoding = tokenizer(input_text, return_tensors=\"pt\", is_split_into_words=False, truncation=True)\n",
    "encoding = {k: v.to(model.device) for k, v in encoding.items()}\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T08:25:14.959061Z",
     "iopub.status.busy": "2024-11-19T08:25:14.957891Z",
     "iopub.status.idle": "2024-11-19T08:25:14.995914Z",
     "shell.execute_reply": "2024-11-19T08:25:14.994703Z",
     "shell.execute_reply.started": "2024-11-19T08:25:14.959003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文本: 今天，美利坚合众国国防部发言人乔治表示中华人民共和国的歼20战机很优秀。\n",
      "预测结果:\n",
      "[CLS]           -> O\n",
      "今               -> O\n",
      "天               -> O\n",
      "，               -> O\n",
      "美               -> B-PLACE\n",
      "利               -> I-PLACE\n",
      "坚               -> I-PLACE\n",
      "合               -> I-PLACE\n",
      "众               -> I-PLACE\n",
      "国               -> I-PLACE\n",
      "国               -> O\n",
      "防               -> O\n",
      "部               -> O\n",
      "发               -> O\n",
      "言               -> O\n",
      "人               -> O\n",
      "乔               -> O\n",
      "治               -> O\n",
      "表               -> O\n",
      "示               -> O\n",
      "中               -> B-PLACE\n",
      "华               -> I-PLACE\n",
      "人               -> I-PLACE\n",
      "民               -> I-PLACE\n",
      "共               -> I-PLACE\n",
      "和               -> I-PLACE\n",
      "国               -> I-PLACE\n",
      "的               -> O\n",
      "歼               -> O\n",
      "20              -> O\n",
      "战               -> O\n",
      "机               -> O\n",
      "很               -> O\n",
      "优               -> O\n",
      "秀               -> O\n",
      "。               -> O\n",
      "[SEP]           -> O\n"
     ]
    }
   ],
   "source": [
    "# 模型预测\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)\n",
    "\n",
    "# 提取预测结果\n",
    "logits = outputs.logits\n",
    "predicted_class_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "# 将预测结果映射为标签，并将标签与原始文本对应起来\n",
    "predicted_labels = [id2label[class_id] for class_id in predicted_class_ids]\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze().tolist())\n",
    "results = list(zip(tokens, predicted_labels))\n",
    "\n",
    "# 打印预测结果\n",
    "print(\"输入文本:\", input_text)\n",
    "print(\"预测结果:\")\n",
    "for token, label in results:\n",
    "    print(f\"{token:15} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设需要预测的文本在一个 DataFrame 中\n",
    "texts = [\n",
    "    \"日本首相菅义伟说日本将继续与中国合作。\",\n",
    "    \"今天，美利坚合众国国防部发言人乔治说中华人民共和国的歼20战机很优秀。\",\n",
    "    \"美国总统拜登说美国将继续支持乌克兰。\",\n",
    "    \"中国国家主席习近平说中国将继续推进全球化。\",\n",
    "    \"俄罗斯总统普京说俄罗斯将继续支持叙利亚。\",\n",
    "]\n",
    "df = pd.DataFrame(data={'text':texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "best_ckpt_path = 'model/checkpoint-78'  # 自己切换为训练得到的最佳模型路径\n",
    "model = AutoModelForTokenClassification.from_pretrained(best_ckpt_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(best_ckpt_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于预测的函数\n",
    "def predict(texts):\n",
    "    # 对每个文本进行tokenization\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH)\n",
    "    \n",
    "    # 将模型放到评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # 获取模型输出 (logits)\n",
    "        outputs = model(**encodings)\n",
    "        \n",
    "    # 获取预测结果\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # 根据最大概率选择预测的标签\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # 移除 [CLS] 和 [SEP] token 的预测结果\n",
    "    # [CLS] 是第一个token， [SEP] 是最后一个token\n",
    "    # 因此在进行预测后，需要去掉第一个和最后一个预测\n",
    "    input_ids = encodings['input_ids']\n",
    "    filtered_predictions = []\n",
    "    \n",
    "    for i, input_id in enumerate(input_ids):\n",
    "        # 获取当前句子的实际token部分（去掉[CLS]和[SEP]）\n",
    "        valid_pred = predictions[i][1:-1]  # 移除第一个和最后一个预测\n",
    "        filtered_predictions.append(valid_pred)\n",
    "    \n",
    "    return filtered_predictions\n",
    "\n",
    "# 执行预测\n",
    "df['predictions'] = predict(df['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 日本首相菅义伟说日本将继续与中国合作。\n",
      "Predicted Labels: [1, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Text: 今天，美利坚合众国国防部发言人乔治说中华人民共和国的歼20战机很优秀。\n",
      "Predicted Labels: [0, 0, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Text: 美国总统拜登说美国将继续支持乌克兰。\n",
      "Predicted Labels: [1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0]\n",
      "Text: 中国国家主席习近平说中国将继续推进全球化。\n",
      "Predicted Labels: [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Text: 俄罗斯总统普京说俄罗斯将继续支持叙利亚。\n",
      "Predicted Labels: [1, 2, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# 输出结果 (根据模型标签数量，可以进一步解码为标签名称)\n",
    "for i, text in enumerate(df['text']):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Labels: {df.loc[i,'predictions'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 日本首相菅义伟说日本将继续与中国合作。\n",
      "Predicted Labels:\n",
      "日               -> B-PLACE\n",
      "本               -> I-PLACE\n",
      "首               -> O\n",
      "相               -> O\n",
      "菅               -> O\n",
      "义               -> O\n",
      "伟               -> O\n",
      "说               -> O\n",
      "日               -> B-PLACE\n",
      "本               -> I-PLACE\n",
      "将               -> O\n",
      "继               -> O\n",
      "续               -> O\n",
      "与               -> O\n",
      "中               -> B-PLACE\n",
      "国               -> I-PLACE\n",
      "合               -> O\n",
      "作               -> O\n",
      "。               -> O\n",
      "\n",
      "\n",
      "Text: 今天，美利坚合众国国防部发言人乔治说中华人民共和国的歼20战机很优秀。\n",
      "Predicted Labels:\n",
      "今               -> O\n",
      "天               -> O\n",
      "，               -> O\n",
      "美               -> B-PLACE\n",
      "利               -> I-PLACE\n",
      "坚               -> I-PLACE\n",
      "合               -> I-PLACE\n",
      "众               -> I-PLACE\n",
      "国               -> I-PLACE\n",
      "国               -> O\n",
      "防               -> O\n",
      "部               -> O\n",
      "发               -> O\n",
      "言               -> O\n",
      "人               -> O\n",
      "乔               -> O\n",
      "治               -> O\n",
      "说               -> O\n",
      "中               -> B-PLACE\n",
      "华               -> I-PLACE\n",
      "人               -> I-PLACE\n",
      "民               -> I-PLACE\n",
      "共               -> I-PLACE\n",
      "和               -> I-PLACE\n",
      "国               -> I-PLACE\n",
      "的               -> O\n",
      "歼               -> O\n",
      "20              -> O\n",
      "战               -> O\n",
      "机               -> O\n",
      "很               -> O\n",
      "优               -> O\n",
      "秀               -> O\n",
      "。               -> O\n",
      "\n",
      "\n",
      "Text: 美国总统拜登说美国将继续支持乌克兰。\n",
      "Predicted Labels:\n",
      "美               -> B-PLACE\n",
      "国               -> I-PLACE\n",
      "总               -> O\n",
      "统               -> O\n",
      "拜               -> O\n",
      "登               -> O\n",
      "说               -> O\n",
      "美               -> B-PLACE\n",
      "国               -> I-PLACE\n",
      "将               -> O\n",
      "继               -> O\n",
      "续               -> O\n",
      "支               -> O\n",
      "持               -> O\n",
      "乌               -> B-PLACE\n",
      "克               -> I-PLACE\n",
      "兰               -> I-PLACE\n",
      "。               -> O\n",
      "\n",
      "\n",
      "Text: 中国国家主席习近平说中国将继续推进全球化。\n",
      "Predicted Labels:\n",
      "中               -> B-PLACE\n",
      "国               -> I-PLACE\n",
      "国               -> O\n",
      "家               -> O\n",
      "主               -> O\n",
      "席               -> O\n",
      "习               -> O\n",
      "近               -> O\n",
      "平               -> O\n",
      "说               -> O\n",
      "中               -> B-PLACE\n",
      "国               -> I-PLACE\n",
      "将               -> O\n",
      "继               -> O\n",
      "续               -> O\n",
      "推               -> O\n",
      "进               -> O\n",
      "全               -> O\n",
      "球               -> O\n",
      "化               -> O\n",
      "。               -> O\n",
      "\n",
      "\n",
      "Text: 俄罗斯总统普京说俄罗斯将继续支持叙利亚。\n",
      "Predicted Labels:\n",
      "俄               -> B-PLACE\n",
      "罗               -> I-PLACE\n",
      "斯               -> I-PLACE\n",
      "总               -> O\n",
      "统               -> O\n",
      "普               -> O\n",
      "京               -> O\n",
      "说               -> O\n",
      "俄               -> B-PLACE\n",
      "罗               -> I-PLACE\n",
      "斯               -> I-PLACE\n",
      "将               -> O\n",
      "继               -> O\n",
      "续               -> O\n",
      "支               -> O\n",
      "持               -> O\n",
      "叙               -> B-PLACE\n",
      "利               -> I-PLACE\n",
      "亚               -> I-PLACE\n",
      "。               -> O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    0: 'O',\n",
    "    1: 'B-PLACE',\n",
    "    2: 'I-PLACE'\n",
    "}\n",
    "\n",
    "# 打印预测结果\n",
    "for i, text in enumerate(df['text']):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Predicted Labels:\")\n",
    "    for token, label_id in zip(tokenizer.tokenize(text), df.loc[i,'predictions']):\n",
    "        label = id2label[label_id.item()]\n",
    "        print(f\"{token:15} -> {label}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>日本首相菅义伟说日本将继续与中国合作。</td>\n",
       "      <td>[tensor(1), tensor(2), tensor(0), tensor(0), t...</td>\n",
       "      <td>[B-PLACE, I-PLACE, O, O, O, O, O, O, B-PLACE, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>今天，美利坚合众国国防部发言人乔治说中华人民共和国的歼20战机很优秀。</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(1), t...</td>\n",
       "      <td>[O, O, O, B-PLACE, I-PLACE, I-PLACE, I-PLACE, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>美国总统拜登说美国将继续支持乌克兰。</td>\n",
       "      <td>[tensor(1), tensor(2), tensor(0), tensor(0), t...</td>\n",
       "      <td>[B-PLACE, I-PLACE, O, O, O, O, O, B-PLACE, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>中国国家主席习近平说中国将继续推进全球化。</td>\n",
       "      <td>[tensor(1), tensor(2), tensor(0), tensor(0), t...</td>\n",
       "      <td>[B-PLACE, I-PLACE, O, O, O, O, O, O, O, O, B-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>俄罗斯总统普京说俄罗斯将继续支持叙利亚。</td>\n",
       "      <td>[tensor(1), tensor(2), tensor(2), tensor(0), t...</td>\n",
       "      <td>[B-PLACE, I-PLACE, I-PLACE, O, O, O, O, O, B-P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text  \\\n",
       "0                  日本首相菅义伟说日本将继续与中国合作。   \n",
       "1  今天，美利坚合众国国防部发言人乔治说中华人民共和国的歼20战机很优秀。   \n",
       "2                   美国总统拜登说美国将继续支持乌克兰。   \n",
       "3                中国国家主席习近平说中国将继续推进全球化。   \n",
       "4                 俄罗斯总统普京说俄罗斯将继续支持叙利亚。   \n",
       "\n",
       "                                         predictions  \\\n",
       "0  [tensor(1), tensor(2), tensor(0), tensor(0), t...   \n",
       "1  [tensor(0), tensor(0), tensor(0), tensor(1), t...   \n",
       "2  [tensor(1), tensor(2), tensor(0), tensor(0), t...   \n",
       "3  [tensor(1), tensor(2), tensor(0), tensor(0), t...   \n",
       "4  [tensor(1), tensor(2), tensor(2), tensor(0), t...   \n",
       "\n",
       "                                              labels  \n",
       "0  [B-PLACE, I-PLACE, O, O, O, O, O, O, B-PLACE, ...  \n",
       "1  [O, O, O, B-PLACE, I-PLACE, I-PLACE, I-PLACE, ...  \n",
       "2  [B-PLACE, I-PLACE, O, O, O, O, O, B-PLACE, I-P...  \n",
       "3  [B-PLACE, I-PLACE, O, O, O, O, O, O, O, O, B-P...  \n",
       "4  [B-PLACE, I-PLACE, I-PLACE, O, O, O, O, O, B-P...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转换df['predictions']为标签\n",
    "df['labels'] = df['predictions'].apply(lambda x: [id2label[i.item()] for i in x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 pipeline 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(best_ckpt_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_ckpt_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': 0.97941077, 'index': 1, 'word': '今', 'start': 0, 'end': 1}, {'entity': 'LABEL_0', 'score': 0.99748343, 'index': 2, 'word': '天', 'start': 1, 'end': 2}, {'entity': 'LABEL_0', 'score': 0.99748224, 'index': 3, 'word': '，', 'start': 2, 'end': 3}, {'entity': 'LABEL_1', 'score': 0.9746602, 'index': 4, 'word': '美', 'start': 3, 'end': 4}, {'entity': 'LABEL_2', 'score': 0.9916592, 'index': 5, 'word': '利', 'start': 4, 'end': 5}, {'entity': 'LABEL_2', 'score': 0.9965677, 'index': 6, 'word': '坚', 'start': 5, 'end': 6}, {'entity': 'LABEL_2', 'score': 0.906812, 'index': 7, 'word': '合', 'start': 6, 'end': 7}, {'entity': 'LABEL_2', 'score': 0.993753, 'index': 8, 'word': '众', 'start': 7, 'end': 8}, {'entity': 'LABEL_2', 'score': 0.9981445, 'index': 9, 'word': '国', 'start': 8, 'end': 9}, {'entity': 'LABEL_0', 'score': 0.9979563, 'index': 10, 'word': '国', 'start': 9, 'end': 10}, {'entity': 'LABEL_0', 'score': 0.9983614, 'index': 11, 'word': '防', 'start': 10, 'end': 11}, {'entity': 'LABEL_0', 'score': 0.9994324, 'index': 12, 'word': '部', 'start': 11, 'end': 12}, {'entity': 'LABEL_0', 'score': 0.9995034, 'index': 13, 'word': '发', 'start': 12, 'end': 13}, {'entity': 'LABEL_0', 'score': 0.9996099, 'index': 14, 'word': '言', 'start': 13, 'end': 14}, {'entity': 'LABEL_0', 'score': 0.99967897, 'index': 15, 'word': '人', 'start': 14, 'end': 15}, {'entity': 'LABEL_0', 'score': 0.998808, 'index': 16, 'word': '乔', 'start': 15, 'end': 16}, {'entity': 'LABEL_0', 'score': 0.99887043, 'index': 17, 'word': '治', 'start': 16, 'end': 17}, {'entity': 'LABEL_0', 'score': 0.9993299, 'index': 18, 'word': '表', 'start': 17, 'end': 18}, {'entity': 'LABEL_0', 'score': 0.99920684, 'index': 19, 'word': '示', 'start': 18, 'end': 19}, {'entity': 'LABEL_1', 'score': 0.99556524, 'index': 20, 'word': '中', 'start': 19, 'end': 20}, {'entity': 'LABEL_2', 'score': 0.9886613, 'index': 21, 'word': '华', 'start': 20, 'end': 21}, {'entity': 'LABEL_2', 'score': 0.9964683, 'index': 22, 'word': '人', 'start': 21, 'end': 22}, {'entity': 'LABEL_2', 'score': 0.9975259, 'index': 23, 'word': '民', 'start': 22, 'end': 23}, {'entity': 'LABEL_2', 'score': 0.997586, 'index': 24, 'word': '共', 'start': 23, 'end': 24}, {'entity': 'LABEL_2', 'score': 0.99785006, 'index': 25, 'word': '和', 'start': 24, 'end': 25}, {'entity': 'LABEL_2', 'score': 0.99873155, 'index': 26, 'word': '国', 'start': 25, 'end': 26}, {'entity': 'LABEL_0', 'score': 0.9933843, 'index': 27, 'word': '的', 'start': 26, 'end': 27}, {'entity': 'LABEL_0', 'score': 0.9960901, 'index': 28, 'word': '歼', 'start': 27, 'end': 28}, {'entity': 'LABEL_0', 'score': 0.98626316, 'index': 29, 'word': '20', 'start': 28, 'end': 30}, {'entity': 'LABEL_0', 'score': 0.9965487, 'index': 30, 'word': '战', 'start': 30, 'end': 31}, {'entity': 'LABEL_0', 'score': 0.9967501, 'index': 31, 'word': '机', 'start': 31, 'end': 32}, {'entity': 'LABEL_0', 'score': 0.9971187, 'index': 32, 'word': '很', 'start': 32, 'end': 33}, {'entity': 'LABEL_0', 'score': 0.9971826, 'index': 33, 'word': '优', 'start': 33, 'end': 34}, {'entity': 'LABEL_0', 'score': 0.9963052, 'index': 34, 'word': '秀', 'start': 34, 'end': 35}, {'entity': 'LABEL_0', 'score': 0.9990269, 'index': 35, 'word': '。', 'start': 35, 'end': 36}]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device)\n",
    "text = \"今天，美利坚合众国国防部发言人乔治表示中华人民共和国的歼20战机很优秀。\"\n",
    "ner_results = nlp(text)\n",
    "print(ner_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
