{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T05:42:52.153580Z",
     "iopub.status.busy": "2024-11-19T05:42:52.152872Z",
     "iopub.status.idle": "2024-11-19T05:42:52.161681Z",
     "shell.execute_reply": "2024-11-19T05:42:52.160092Z",
     "shell.execute_reply.started": "2024-11-19T05:42:52.153518Z"
    }
   },
   "source": [
    "ÂèÇËÄÉÔºöhttps://github.com/vaibhavdangar09/NER-WITH-BERT/blob/main/NER_WITH_BERT.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÊèêÂèñÊï∞ÊçÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "dbname = 'other' \n",
    "engine = create_engine(f'postgresql://postgres:123@localhost:5432/{dbname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sql = \"\"\"\n",
    "    SELECT text FROM mfa_cn\n",
    "    UNION\n",
    "    SELECT title as text FROM mfa_usa;\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(text(sql), con=engine)\n",
    "except:\n",
    "    df = pd.read_csv('data/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Â∞ÜÊï∞ÊçÆÂÜôÂÖ• txt\n",
    "- Âà©Áî®Ê†áÊ≥®Â∑•ÂÖ∑Ê†áÊ≥®ÂÆû‰Ωì\n",
    "- ‰ΩøÁî®[MarkStudio](https://github.com/cuiwang/MarkStudio)ËøõË°åÊ†áÊ≥®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Êï∞ÊçÆÂ∑≤ÊàêÂäüÂÜôÂÖ• data/ner_label_in.txt Êñá‰ª∂ÔºÅ\n"
     ]
    }
   ],
   "source": [
    "# Â∞ÜÊØè‰∏ÄË°åÊï∞ÊçÆÂÜôÂÖ•txtÊñá‰ª∂\n",
    "txt_file = 'data/ner_label_in.txt'\n",
    "with open(txt_file, 'w', encoding='utf-8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        f.write(row['text'] + '\\n')  #\n",
    "\n",
    "print(f\"Êï∞ÊçÆÂ∑≤ÊàêÂäüÂÜôÂÖ• {txt_file} Êñá‰ª∂ÔºÅ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.381007Z",
     "iopub.status.idle": "2024-11-19T09:06:29.390589Z",
     "shell.execute_reply": "2024-11-19T09:06:29.389496Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.380979Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 128  # ÊØèÊù°Êï∞ÊçÆÊúÄÂ§ßÈïøÂ∫¶\n",
    "BATCH_SIZE = 8  # ÊâπÂ§ÑÁêÜÂ§ßÂ∞è\n",
    "NUM_LABELS = 3  # NERÊ†áËÆ∞Êï∞Èáè (e.g., B-LOC, I-LOC, O, etc.)\n",
    "MODEL_NAME = 'bert-base-chinese'  # Ê®°ÂûãÂêçÁß∞\n",
    "# MODEL_PATH = 'model/'  # Ê®°ÂûãË∑ØÂæÑ\n",
    "MODEL_PATH = r'E:/JupyterLab//LLM//Large-Model//bert//'  # Ê®°ÂûãË∑ØÂæÑ\n",
    "LABEL_DATA_PATH = 'data/label_data.json'  # Ê†áÊ≥®Êï∞ÊçÆË∑ØÂæÑ\n",
    "OUT_DIR = 'model/'  # ËæìÂá∫Ë∑ØÂæÑ\n",
    "LOG_DIR = 'log/'  # Êó•ÂøóË∑ØÂæÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.392557Z",
     "iopub.status.busy": "2024-11-19T09:06:29.392199Z",
     "iopub.status.idle": "2024-11-19T09:06:29.397763Z",
     "shell.execute_reply": "2024-11-19T09:06:29.396547Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.392527Z"
    }
   },
   "outputs": [],
   "source": [
    "label_list = ['O','B-PLACE','I-PLACE']  # Ê†πÊçÆ‰Ω†Ëá™Â∑±ÁöÑÊ†áËÆ∞ÈõÜÂêàËøõË°å‰øÆÊîπ\n",
    "id2label = {\n",
    "    i: label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: i for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.399794Z",
     "iopub.status.busy": "2024-11-19T09:06:29.399236Z",
     "iopub.status.idle": "2024-11-19T09:06:29.411228Z",
     "shell.execute_reply": "2024-11-19T09:06:29.410132Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.399746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O', 1: 'B-PLACE', 2: 'I-PLACE'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.427442Z",
     "iopub.status.busy": "2024-11-19T09:06:29.427117Z",
     "iopub.status.idle": "2024-11-19T09:06:29.432960Z",
     "shell.execute_reply": "2024-11-19T09:06:29.431897Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.427414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B-PLACE': 1, 'I-PLACE': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Êï∞ÊçÆÂ§ÑÁêÜ\n",
    "\n",
    "- MarkStudioËæìÂá∫ÁöÑÊ†ºÂºè‰∏∫JOSNÊñá‰ª∂\n",
    "- Êàë‰ª¨È¢ÑÂèñÁöÑÊï∞ÊçÆÊ†ºÂºè‰∏∫Ôºö\n",
    "\n",
    "    ```python\n",
    "    text: ['Â§ñ‰∫§ÈÉ®‰∏≠ÈòøÂêà‰ΩúËÆ∫Âùõ‰∫ãÂä°Â§ß‰ΩøÊùéÁêõËÆøÈóÆÂç°Â°îÂ∞î']\n",
    "    label: [0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2]\n",
    "    ```\n",
    "- Êàë‰ª¨‰ΩøÁî® [BIO](https://blog.csdn.net/HappyRocking/article/details/79716212) Ê≥ïÂàôÊ†áÊ≥®Êï∞ÊçÆ\n",
    "- 0,1,2ÂàÜÂà´Ë°®Á§∫ÈùûÂÆû‰ΩìÔºåÂÆû‰ΩìÂºÄÂßãÔºåÂÆû‰Ωì‰∏≠Èó¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:29.436306Z",
     "iopub.status.busy": "2024-11-19T09:06:29.435974Z",
     "iopub.status.idle": "2024-11-19T09:06:30.434850Z",
     "shell.execute_reply": "2024-11-19T09:06:30.434141Z",
     "shell.execute_reply.started": "2024-11-19T09:06:29.436278Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.438930Z",
     "iopub.status.busy": "2024-11-19T09:06:30.438733Z",
     "iopub.status.idle": "2024-11-19T09:06:30.449944Z",
     "shell.execute_reply": "2024-11-19T09:06:30.449283Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.438909Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: ['ÊãúÁôªÊÄªÁªüÂØπÂõΩ‰ºö‰∏§Èô¢ËÅîÂ∏≠‰ºöËÆÆÂèëË°®ËÆ≤ËØù', '‰∏≠ÂõΩÊîøÂ∫ú‰∏≠‰∏úÈóÆÈ¢òÁâπ‰ΩøÁøüÈöΩÂá∫Â∏≠ÈáëÁ†ñÂõΩÂÆ∂‰∏≠‰∏ú‰∫ãÂä°ÂâØÂ§ñÈïø/Áâπ‰ΩøÁ£ãÂïÜ']\n",
      "Labels: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 2, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Êù•Ëá™Ê†áÊ≥®Â•ΩÁöÑJSONÊñá‰ª∂\n",
    "with open(LABEL_DATA_PATH, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "for entry in data:\n",
    "    text = entry['content']\n",
    "    label_sequence = ['O'] * len(text)  # ÂàùÂßãÂåñÊâÄÊúâÂ≠óÁ¨¶ÁöÑÊ†áÁ≠æ‰∏∫ 'O'\n",
    "\n",
    "    for tag in entry['tags']:\n",
    "        if tag['name'] == 'PLACE':\n",
    "            start = tag['start']\n",
    "            end = tag['end']\n",
    "\n",
    "            # Â∞ÜÂºÄÂßã‰ΩçÁΩÆÊ†áËÆ∞‰∏∫ 'B-PLACE'\n",
    "            label_sequence[start] = 'B-PLACE'\n",
    "\n",
    "            # Â∞ÜÂêéÁª≠‰ΩçÁΩÆÊ†áËÆ∞‰∏∫ 'I-PLACE'\n",
    "            for i in range(start + 1, end):\n",
    "                label_sequence[i] = 'I-PLACE'\n",
    "\n",
    "    # Â∞ÜÊ†áÁ≠æËΩ¨Êç¢‰∏∫Ê†áÁ≠æÁ¥¢Âºï\n",
    "    label_indices = [label2id[label] for label in label_sequence]\n",
    "\n",
    "    texts.append(text)\n",
    "    labels.append(label_indices)\n",
    "\n",
    "# Ê£ÄÊü•ËΩ¨Êç¢ÂêéÁöÑÊ†ºÂºè\n",
    "print(\"Texts:\", texts[-2:])\n",
    "print(\"Labels:\", labels[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ËøòÈúÄË¶ÅÂØπÊï∞ÊçÆËøõË°åÂ§ÑÁêÜ\n",
    "- È¶ñÂÖàÊòØËøõË°åÊï∞ÊçÆÈõÜÂàíÂàÜ\n",
    "- ÁÑ∂ÂêéÊûÑÂª∫‰∏∫‰ª•‰∏ãÂ≠óÂÖ∏ÂΩ¢ÂºèÔºö\n",
    "```python\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['id', 'tokens', 'ner_tags'],\n",
    "        num_rows: 305\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['id', 'tokens', 'ner_tags'],\n",
    "        num_rows: 38\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['id', 'tokens', 'ner_tags'],\n",
    "        num_rows: 39\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.450709Z",
     "iopub.status.busy": "2024-11-19T09:06:30.450526Z",
     "iopub.status.idle": "2024-11-19T09:06:30.455718Z",
     "shell.execute_reply": "2024-11-19T09:06:30.455075Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.450690Z"
    }
   },
   "outputs": [],
   "source": [
    "# ÂàíÂàÜÊï∞ÊçÆÈõÜ--ËÆ≠ÁªÉÊµãËØïÂíåÈ™åËØÅ\n",
    "texts_train, texts_temp, labels_train, labels_temp = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "texts_val, texts_test, labels_val, labels_test = train_test_split(\n",
    "    texts_temp, labels_temp, test_size=0.5, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.456570Z",
     "iopub.status.busy": "2024-11-19T09:06:30.456385Z",
     "iopub.status.idle": "2024-11-19T09:06:30.461436Z",
     "shell.execute_reply": "2024-11-19T09:06:30.460887Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.456551Z"
    }
   },
   "outputs": [],
   "source": [
    "# ÊûÑÈÄ†Â≠óÂÖ∏ÂΩ¢ÂºèÁöÑÊï∞ÊçÆ\n",
    "def create_dataset(texts, labels):\n",
    "    ids = list(range(len(texts)))\n",
    "    tokens_list = [list(text) for text in texts]\n",
    "    return {'id': ids, 'tokens': tokens_list, 'ner_tags': labels}\n",
    "\n",
    "train_data = create_dataset(texts_train, labels_train)\n",
    "val_data = create_dataset(texts_val, labels_val)\n",
    "test_data = create_dataset(texts_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.490090Z",
     "iopub.status.busy": "2024-11-19T09:06:30.489798Z",
     "iopub.status.idle": "2024-11-19T09:06:30.511792Z",
     "shell.execute_reply": "2024-11-19T09:06:30.511245Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.490068Z"
    }
   },
   "outputs": [],
   "source": [
    "# ÂàõÂª∫ Dataset Âíå DatasetDict\n",
    "train_dataset = Dataset.from_dict(train_data)\n",
    "val_dataset = Dataset.from_dict(val_data)\n",
    "test_dataset = Dataset.from_dict(test_data)\n",
    "\n",
    "ner_data = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.687147Z",
     "iopub.status.busy": "2024-11-19T09:06:30.686411Z",
     "iopub.status.idle": "2024-11-19T09:06:30.692636Z",
     "shell.execute_reply": "2024-11-19T09:06:30.691501Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.687123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 305\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 38\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 39\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:30.880319Z",
     "iopub.status.busy": "2024-11-19T09:06:30.879365Z",
     "iopub.status.idle": "2024-11-19T09:06:30.889726Z",
     "shell.execute_reply": "2024-11-19T09:06:30.888593Z",
     "shell.execute_reply.started": "2024-11-19T09:06:30.880267Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'tokens': ['Áæé',\n",
       "  'ÂõΩ',\n",
       "  'ÂÆ£',\n",
       "  'Â∏É',\n",
       "  'Âêë',\n",
       "  'Âä†',\n",
       "  'Ê≤ô',\n",
       "  'Âèä',\n",
       "  'ËØ•',\n",
       "  'Âú∞',\n",
       "  'Âå∫',\n",
       "  'ÁöÑ',\n",
       "  'Â∑¥',\n",
       "  'Âãí',\n",
       "  'ÊñØ',\n",
       "  'Âù¶',\n",
       "  'Âπ≥',\n",
       "  'Ê∞ë',\n",
       "  'Êèê',\n",
       "  '‰æõ',\n",
       "  'Êõ¥',\n",
       "  'Â§ö',\n",
       "  '‰∫∫',\n",
       "  'ÈÅì',\n",
       "  '‰∏ª',\n",
       "  '‰πâ',\n",
       "  'Êè¥',\n",
       "  'Âä©'],\n",
       " 'ner_tags': [1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÁºñÁ†Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:31.616927Z",
     "iopub.status.busy": "2024-11-19T09:06:31.615885Z",
     "iopub.status.idle": "2024-11-19T09:06:33.096302Z",
     "shell.execute_reply": "2024-11-19T09:06:33.095337Z",
     "shell.execute_reply.started": "2024-11-19T09:06:31.616874Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.098043Z",
     "iopub.status.busy": "2024-11-19T09:06:33.097738Z",
     "iopub.status.idle": "2024-11-19T09:06:33.147647Z",
     "shell.execute_reply": "2024-11-19T09:06:33.146805Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.098022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='E:/JupyterLab//LLM//Large-Model//bert//bert-base-chinese', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH+MODEL_NAME)  # Ëá™Â∑±‰∏ãËΩΩÁöÑ‰∏≠Êñá BERT Ê®°Âûã\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.148810Z",
     "iopub.status.busy": "2024-11-19T09:06:33.148521Z",
     "iopub.status.idle": "2024-11-19T09:06:33.154558Z",
     "shell.execute_reply": "2024-11-19T09:06:33.153758Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.148790Z"
    }
   },
   "outputs": [],
   "source": [
    "example_text = ner_data['train'][0]\n",
    "tokenized_input = tokenizer(example_text['tokens'],is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "word_ids = tokenized_input.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.156157Z",
     "iopub.status.busy": "2024-11-19T09:06:33.155847Z",
     "iopub.status.idle": "2024-11-19T09:06:33.160933Z",
     "shell.execute_reply": "2024-11-19T09:06:33.160116Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.156136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 5401, 1744, 2146, 2357, 1403, 1217, 3763, 1350, 6421, 1765, 1277, 4638, 2349, 1239, 3172, 1788, 2398, 3696, 2990, 897, 3291, 1914, 782, 6887, 712, 721, 3001, 1221, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "['[CLS]', 'Áæé', 'ÂõΩ', 'ÂÆ£', 'Â∏É', 'Âêë', 'Âä†', 'Ê≤ô', 'Âèä', 'ËØ•', 'Âú∞', 'Âå∫', 'ÁöÑ', 'Â∑¥', 'Âãí', 'ÊñØ', 'Âù¶', 'Âπ≥', 'Ê∞ë', 'Êèê', '‰æõ', 'Êõ¥', 'Â§ö', '‰∫∫', 'ÈÅì', '‰∏ª', '‰πâ', 'Êè¥', 'Âä©', '[SEP]']\n",
      "\n",
      "\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input)\n",
    "print(\"\\n\")\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.161837Z",
     "iopub.status.busy": "2024-11-19T09:06:33.161653Z",
     "iopub.status.idle": "2024-11-19T09:06:33.166600Z",
     "shell.execute_reply": "2024-11-19T09:06:33.165776Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.161817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the tokens is : 30\n",
      "Length of the ner tags is: 28\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the tokens is : {len(tokens)}')\n",
    "print(f'Length of the ner tags is: {len(ner_data[\"train\"][0][\"ner_tags\"])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Âú®ËøôÈáåÔºåÊñáÂ≠óÊ†áËÆ∞ÂíåtokenÁöÑÂ∞∫ÂØ∏ÊòØ‰∏çÂêåÁöÑÔºåÊâÄ‰ª•‰∏∫‰∫Ü‰Ωø‰ª£Â∏ÅÂíåÊñáÂ≠óÊ†áËÆ∞ÁöÑÂ∞∫ÂØ∏Áõ∏ÂêåÔºåÊàë‰ª¨Âú®ÊñáÂ≠óÊ†áËÆ∞ÁöÑÁ¨¨‰∏Ä‰∏™ÂíåÊúÄÂêé‰∏Ä‰∏™‰ΩçÁΩÆÂä†‰∏ä -100„ÄÇ\n",
    "- Âú®ËÆ≠ÁªÉËøáÁ®ã‰∏≠ÔºåBERT Ê®°ÂûãÈÅøÂºÄ‰∫Ü -100„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.167522Z",
     "iopub.status.busy": "2024-11-19T09:06:33.167337Z",
     "iopub.status.idle": "2024-11-19T09:06:33.173289Z",
     "shell.execute_reply": "2024-11-19T09:06:33.172490Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.167503Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        # Special tokens like `` and `<\\s>` are originally mapped to None\n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # set ‚Äì100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                # mask the subword representations after the first subword\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.641287Z",
     "iopub.status.busy": "2024-11-19T09:06:33.640385Z",
     "iopub.status.idle": "2024-11-19T09:06:33.648709Z",
     "shell.execute_reply": "2024-11-19T09:06:33.647536Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.641234Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1744, 2157, 2128, 1059, 7560, 7309, 3345, 1046, 185, 3763, 1164, 3152, 1068, 754, 915, 5384, 3172, 1415, 1104, 5468, 1394, 1744, 2128, 4415, 833, 517, 1912, 2231, 4958, 7313, 3340, 5276, 518, 1104, 6379, 4638, 1898, 3209, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = tokenize_and_align_labels(ner_data['train'][1:2])\n",
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:33.831270Z",
     "iopub.status.busy": "2024-11-19T09:06:33.830355Z",
     "iopub.status.idle": "2024-11-19T09:06:33.838701Z",
     "shell.execute_reply": "2024-11-19T09:06:33.837478Z",
     "shell.execute_reply.started": "2024-11-19T09:06:33.831217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "ÂõΩ_______________________________________ 0\n",
      "ÂÆ∂_______________________________________ 0\n",
      "ÂÆâ_______________________________________ 0\n",
      "ÂÖ®_______________________________________ 0\n",
      "È°æ_______________________________________ 0\n",
      "ÈóÆ_______________________________________ 0\n",
      "Êù∞_______________________________________ 0\n",
      "ÂÖã_______________________________________ 0\n",
      "¬∑_______________________________________ 0\n",
      "Ê≤ô_______________________________________ 0\n",
      "Âà©_______________________________________ 0\n",
      "Êñá_______________________________________ 0\n",
      "ÂÖ≥_______________________________________ 0\n",
      "‰∫é_______________________________________ 0\n",
      "‰øÑ_______________________________________ 1\n",
      "ÁΩó_______________________________________ 2\n",
      "ÊñØ_______________________________________ 2\n",
      "Âê¶_______________________________________ 0\n",
      "ÂÜ≥_______________________________________ 0\n",
      "ËÅî_______________________________________ 0\n",
      "Âêà_______________________________________ 0\n",
      "ÂõΩ_______________________________________ 0\n",
      "ÂÆâ_______________________________________ 0\n",
      "ÁêÜ_______________________________________ 0\n",
      "‰ºö_______________________________________ 0\n",
      "„Ää_______________________________________ 0\n",
      "Â§ñ_______________________________________ 0\n",
      "Â±Ç_______________________________________ 0\n",
      "Á©∫_______________________________________ 0\n",
      "Èó¥_______________________________________ 0\n",
      "Êù°_______________________________________ 0\n",
      "Á∫¶_______________________________________ 0\n",
      "„Äã_______________________________________ 0\n",
      "ÂÜ≥_______________________________________ 0\n",
      "ËÆÆ_______________________________________ 0\n",
      "ÁöÑ_______________________________________ 0\n",
      "Â£∞_______________________________________ 0\n",
      "Êòé_______________________________________ 0\n",
      "[SEP]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(example_text[\"input_ids\"][0]), example_text[\"labels\"][0]):\n",
    "    print(f\"{token:_<40} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:34.008303Z",
     "iopub.status.busy": "2024-11-19T09:06:34.008033Z",
     "iopub.status.idle": "2024-11-19T09:06:34.164409Z",
     "shell.execute_reply": "2024-11-19T09:06:34.163382Z",
     "shell.execute_reply.started": "2024-11-19T09:06:34.008282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7debbb3f23e0487ca9ca92839b94146a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/305 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2f8914e7044d6d95cd9496d9c1fe4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4a8a9adbc314d6e8eb248013c7c01d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/39 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Â∫îÁî®‰∫éÊï¥‰∏™Êï∞ÊçÆ\n",
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:34.207791Z",
     "iopub.status.busy": "2024-11-19T09:06:34.207059Z",
     "iopub.status.idle": "2024-11-19T09:06:34.218545Z",
     "shell.execute_reply": "2024-11-19T09:06:34.217368Z",
     "shell.execute_reply.started": "2024-11-19T09:06:34.207753Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'tokens': ['Áæé',\n",
       "  'ÂõΩ',\n",
       "  'ÂÆ£',\n",
       "  'Â∏É',\n",
       "  'Âêë',\n",
       "  'Âä†',\n",
       "  'Ê≤ô',\n",
       "  'Âèä',\n",
       "  'ËØ•',\n",
       "  'Âú∞',\n",
       "  'Âå∫',\n",
       "  'ÁöÑ',\n",
       "  'Â∑¥',\n",
       "  'Âãí',\n",
       "  'ÊñØ',\n",
       "  'Âù¶',\n",
       "  'Âπ≥',\n",
       "  'Ê∞ë',\n",
       "  'Êèê',\n",
       "  '‰æõ',\n",
       "  'Êõ¥',\n",
       "  'Â§ö',\n",
       "  '‰∫∫',\n",
       "  'ÈÅì',\n",
       "  '‰∏ª',\n",
       "  '‰πâ',\n",
       "  'Êè¥',\n",
       "  'Âä©'],\n",
       " 'ner_tags': [1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids': [101,\n",
       "  5401,\n",
       "  1744,\n",
       "  2146,\n",
       "  2357,\n",
       "  1403,\n",
       "  1217,\n",
       "  3763,\n",
       "  1350,\n",
       "  6421,\n",
       "  1765,\n",
       "  1277,\n",
       "  4638,\n",
       "  2349,\n",
       "  1239,\n",
       "  3172,\n",
       "  1788,\n",
       "  2398,\n",
       "  3696,\n",
       "  2990,\n",
       "  897,\n",
       "  3291,\n",
       "  1914,\n",
       "  782,\n",
       "  6887,\n",
       "  712,\n",
       "  721,\n",
       "  3001,\n",
       "  1221,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÂÆö‰πâÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:34.865674Z",
     "iopub.status.busy": "2024-11-19T09:06:34.864681Z",
     "iopub.status.idle": "2024-11-19T09:06:35.741734Z",
     "shell.execute_reply": "2024-11-19T09:06:35.740789Z",
     "shell.execute_reply.started": "2024-11-19T09:06:34.865620Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:35.751682Z",
     "iopub.status.busy": "2024-11-19T09:06:35.751472Z",
     "iopub.status.idle": "2024-11-19T09:06:36.359175Z",
     "shell.execute_reply": "2024-11-19T09:06:36.357967Z",
     "shell.execute_reply.started": "2024-11-19T09:06:35.751661Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at E:/JupyterLab//LLM//Large-Model//bert//bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ÂàùÂßãÂåñÊ®°Âûã\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_PATH+MODEL_NAME, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ËÆ≠ÁªÉ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:36.361453Z",
     "iopub.status.busy": "2024-11-19T09:06:36.361067Z",
     "iopub.status.idle": "2024-11-19T09:06:36.389382Z",
     "shell.execute_reply": "2024-11-19T09:06:36.388497Z",
     "shell.execute_reply.started": "2024-11-19T09:06:36.361430Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AdamW\n",
    "from transformers import DataCollatorForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:36.619080Z",
     "iopub.status.busy": "2024-11-19T09:06:36.618750Z",
     "iopub.status.idle": "2024-11-19T09:06:36.628825Z",
     "shell.execute_reply": "2024-11-19T09:06:36.627671Z",
     "shell.execute_reply.started": "2024-11-19T09:06:36.619058Z"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_ner_metrics(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "    Ëá™ÂÆö‰πâËØÑ‰º∞ÂáΩÊï∞ÔºåËæìÂÖ•‰∏∫‰∫åÁª¥ÂàóË°®ÔºåËæìÂá∫‰∏∫ÂêÑÊåáÊ†á\n",
    "    \"\"\"\n",
    "    assert len(true_labels) == len(pred_labels), \"true_labels Âíå pred_labels ÁöÑÈïøÂ∫¶ÂøÖÈ°ª‰∏ÄËá¥\"\n",
    "    \n",
    "    # ÂàùÂßãÂåñÁªüËÆ°ÂèòÈáè\n",
    "    total_true = 0  # ÊÄªÁöÑÁúüÂÆûÂÆû‰ΩìÊï∞\n",
    "    total_pred = 0  # ÊÄªÁöÑÈ¢ÑÊµãÂÆû‰ΩìÊï∞\n",
    "    total_correct = 0  # È¢ÑÊµãÊ≠£Á°ÆÁöÑÂÆû‰ΩìÊï∞\n",
    "    total_tokens = 0  # ÊÄªÁöÑÊ†áÊ≥®ÁöÑtokenÊï∞\n",
    "    correct_tokens = 0  # È¢ÑÊµãÊ≠£Á°ÆÁöÑtokenÊï∞\n",
    "    \n",
    "    # ÈÅçÂéÜÊØè‰∏™Â∫èÂàó\n",
    "    for true_seq, pred_seq in zip(true_labels, pred_labels):\n",
    "        assert len(true_seq) == len(pred_seq), \"ÊØè‰∏™Â∫èÂàóÁöÑÈïøÂ∫¶ÂøÖÈ°ª‰∏ÄËá¥\"\n",
    "        \n",
    "        for true, pred in zip(true_seq, pred_seq):\n",
    "            # ÁªüËÆ° token-level ÂáÜÁ°ÆÊÄß\n",
    "            total_tokens += 1\n",
    "            if true == pred:\n",
    "                correct_tokens += 1\n",
    "            \n",
    "            # Â¶ÇÊûúÊòØÂÆû‰ΩìÊ†áÁ≠æÔºåÊõ¥Êñ∞ÁªüËÆ°\n",
    "            if true != \"O\":  # ÁúüÂÆûÊ†áÁ≠æ‰∏∫ÂÆû‰Ωì\n",
    "                total_true += 1\n",
    "                if true == pred:  # È¢ÑÊµãÊ≠£Á°ÆÁöÑÂÆû‰Ωì\n",
    "                    total_correct += 1\n",
    "            \n",
    "            if pred != \"O\":  # È¢ÑÊµãÊ†áÁ≠æ‰∏∫ÂÆû‰Ωì\n",
    "                total_pred += 1\n",
    "    \n",
    "    # ËÆ°ÁÆóÊåáÊ†á\n",
    "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "    precision = total_correct / total_pred if total_pred > 0 else 0.0\n",
    "    recall = total_correct / total_true if total_true > 0 else 0.0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits, labels = pred\n",
    "    pred_logits = pred_logits.argmax(-1)\n",
    "    # ÂèñÂéªÈô§ padding ÁöÑÈÉ®ÂàÜ\n",
    "    predictions = [\n",
    "        [id2label[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "        [id2label[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(pred_logits, labels)\n",
    "   ]\n",
    "    result = calculate_ner_metrics(\n",
    "        true_labels,\n",
    "        predictions\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:37.497242Z",
     "iopub.status.busy": "2024-11-19T09:06:37.496128Z",
     "iopub.status.idle": "2024-11-19T09:06:37.614202Z",
     "shell.execute_reply": "2024-11-19T09:06:37.612987Z",
     "shell.execute_reply.started": "2024-11-19T09:06:37.497186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ÈáçÂÜô Trainer Á±ª\n",
    "class CustomTrainer(Trainer):\n",
    "    def create_optimizer(self):\n",
    "        if self.optimizer is None:\n",
    "            # Ëé∑ÂèñÊ®°ÂûãÂèÇÊï∞\n",
    "            decay_parameters = [\n",
    "                p for n, p in self.model.named_parameters() if n.endswith(\"weight\")\n",
    "            ]\n",
    "            no_decay_parameters = [\n",
    "                p for n, p in self.model.named_parameters() if n.endswith(\"bias\")\n",
    "            ]\n",
    "            # Â∞ÜÂèÇÊï∞ÂàÜÁªÑ\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\"params\": decay_parameters, \"weight_decay\": self.args.weight_decay},\n",
    "                {\"params\": no_decay_parameters, \"weight_decay\": 0.0},\n",
    "            ]\n",
    "            # ‰ΩøÁî® AdamW ‰Ωú‰∏∫‰ºòÂåñÂô®\n",
    "            self.optimizer = AdamW(\n",
    "                optimizer_grouped_parameters, lr=self.args.learning_rate\n",
    "            )\n",
    "        return self.optimizer\n",
    "\n",
    "\n",
    "# ÂàõÂª∫ËÆ≠ÁªÉÂèÇÊï∞\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=LOG_DIR,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:38.072437Z",
     "iopub.status.busy": "2024-11-19T09:06:38.071443Z",
     "iopub.status.idle": "2024-11-19T09:06:38.077693Z",
     "shell.execute_reply": "2024-11-19T09:06:38.076486Z",
     "shell.execute_reply.started": "2024-11-19T09:06:38.072383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Êï∞ÊçÆÊî∂ÈõÜÂô®ÔºåÁî®‰∫éÂ∞ÜÊï∞ÊçÆËΩ¨Êç¢‰∏∫Ê®°ÂûãÂèØÊé•ÂèóÁöÑÊ†ºÂºè\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:38.973147Z",
     "iopub.status.busy": "2024-11-19T09:06:38.972111Z",
     "iopub.status.idle": "2024-11-19T09:06:39.266938Z",
     "shell.execute_reply": "2024-11-19T09:06:39.265972Z",
     "shell.execute_reply.started": "2024-11-19T09:06:38.973093Z"
    }
   },
   "outputs": [],
   "source": [
    "# ÂÆö‰πâ Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,  # ÊõøÊç¢‰∏∫‰Ω†ÁöÑÊ®°Âûã\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:56.918883Z",
     "iopub.status.busy": "2024-11-19T09:06:56.918387Z",
     "iopub.status.idle": "2024-11-19T09:06:56.927024Z",
     "shell.execute_reply": "2024-11-19T09:06:56.925775Z",
     "shell.execute_reply.started": "2024-11-19T09:06:56.918859Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T09:06:59.405520Z",
     "iopub.status.busy": "2024-11-19T09:06:59.404843Z",
     "iopub.status.idle": "2024-11-19T09:07:01.112018Z",
     "shell.execute_reply": "2024-11-19T09:07:01.110809Z",
     "shell.execute_reply.started": "2024-11-19T09:06:59.405466Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e44f4993c24335b7d0067a1c22621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\py311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f38517987ba6429f8c394cde74d83855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.029824865981936455, 'eval_accuracy': 0.993993993993994, 'eval_precision': 0.9854368932038835, 'eval_recall': 0.9854368932038835, 'eval_f1_score': 0.9854368932038835, 'eval_runtime': 0.1217, 'eval_samples_per_second': 312.205, 'eval_steps_per_second': 41.08, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88ec27d9e9d4362b7a74b1cb5f9c124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.01185962837189436, 'eval_accuracy': 0.998998998998999, 'eval_precision': 0.9951690821256038, 'eval_recall': 1.0, 'eval_f1_score': 0.9975786924939467, 'eval_runtime': 0.1151, 'eval_samples_per_second': 330.147, 'eval_steps_per_second': 43.44, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1074cf9ca34fc8bf41a05009e5d943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012030001729726791, 'eval_accuracy': 0.995995995995996, 'eval_precision': 0.9950980392156863, 'eval_recall': 0.9854368932038835, 'eval_f1_score': 0.9902439024390244, 'eval_runtime': 0.1166, 'eval_samples_per_second': 325.959, 'eval_steps_per_second': 42.889, 'epoch': 3.0}\n",
      "{'train_runtime': 19.4364, 'train_samples_per_second': 47.077, 'train_steps_per_second': 6.02, 'train_loss': 0.10041725941193409, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=117, training_loss=0.10041725941193409, metrics={'train_runtime': 19.4364, 'train_samples_per_second': 47.077, 'train_steps_per_second': 6.02, 'total_flos': 19394825045526.0, 'train_loss': 0.10041725941193409, 'epoch': 3.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ËÆ≠ÁªÉ model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T08:17:45.058370Z",
     "iopub.status.busy": "2024-11-19T08:17:45.057797Z",
     "iopub.status.idle": "2024-11-19T08:17:45.063345Z",
     "shell.execute_reply": "2024-11-19T08:17:45.062571Z",
     "shell.execute_reply.started": "2024-11-19T08:17:45.058349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model/checkpoint-78'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_ckpt_path = trainer.state.best_model_checkpoint\n",
    "best_ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ËØÑ‰º∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T08:18:35.309711Z",
     "iopub.status.busy": "2024-11-19T08:18:35.309055Z",
     "iopub.status.idle": "2024-11-19T08:18:35.520661Z",
     "shell.execute_reply": "2024-11-19T08:18:35.519495Z",
     "shell.execute_reply.started": "2024-11-19T08:18:35.309656Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82354823ef040a19d9cb9be004b4d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0154577000066638,\n",
       " 'eval_accuracy': 0.9952516619183286,\n",
       " 'eval_precision': 0.9822485207100592,\n",
       " 'eval_recall': 0.9880952380952381,\n",
       " 'eval_f1_score': 0.9851632047477745,\n",
       " 'eval_runtime': 0.1442,\n",
       " 'eval_samples_per_second': 270.544,\n",
       " 'eval_steps_per_second': 34.685,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ÊµãËØï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T08:27:10.144699Z",
     "iopub.status.busy": "2024-11-19T08:27:10.144123Z",
     "iopub.status.idle": "2024-11-19T08:27:10.156436Z",
     "shell.execute_reply": "2024-11-19T08:27:10.155045Z",
     "shell.execute_reply.started": "2024-11-19T08:27:10.144644Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'‰ªäÂ§©ÔºåÁæéÂà©ÂùöÂêà‰ºóÂõΩÂõΩÈò≤ÈÉ®ÂèëË®Ä‰∫∫‰πîÊ≤ªË°®Á§∫‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊ≠º20ÊàòÊú∫Âæà‰ºòÁßÄ„ÄÇ'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ÊµãËØïÊñáÊú¨\n",
    "input_text = \"‰ªäÂ§©ÔºåÁæéÂà©ÂùöÂêà‰ºóÂõΩÂõΩÈò≤ÈÉ®ÂèëË®Ä‰∫∫‰πîÊ≤ªË°®Á§∫‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊ≠º20ÊàòÊú∫Âæà‰ºòÁßÄ„ÄÇ\"\n",
    "encoding = tokenizer(input_text, return_tensors=\"pt\", is_split_into_words=False, truncation=True)\n",
    "encoding = {k: v.to(model.device) for k, v in encoding.items()}\n",
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T08:25:14.959061Z",
     "iopub.status.busy": "2024-11-19T08:25:14.957891Z",
     "iopub.status.idle": "2024-11-19T08:25:14.995914Z",
     "shell.execute_reply": "2024-11-19T08:25:14.994703Z",
     "shell.execute_reply.started": "2024-11-19T08:25:14.959003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ËæìÂÖ•ÊñáÊú¨: ‰ªäÂ§©ÔºåÁæéÂà©ÂùöÂêà‰ºóÂõΩÂõΩÈò≤ÈÉ®ÂèëË®Ä‰∫∫‰πîÊ≤ªË°®Á§∫‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊ≠º20ÊàòÊú∫Âæà‰ºòÁßÄ„ÄÇ\n",
      "È¢ÑÊµãÁªìÊûú:\n",
      "[CLS]           -> O\n",
      "‰ªä               -> O\n",
      "Â§©               -> O\n",
      "Ôºå               -> O\n",
      "Áæé               -> B-PLACE\n",
      "Âà©               -> I-PLACE\n",
      "Âùö               -> I-PLACE\n",
      "Âêà               -> I-PLACE\n",
      "‰ºó               -> I-PLACE\n",
      "ÂõΩ               -> I-PLACE\n",
      "ÂõΩ               -> O\n",
      "Èò≤               -> O\n",
      "ÈÉ®               -> O\n",
      "Âèë               -> O\n",
      "Ë®Ä               -> O\n",
      "‰∫∫               -> O\n",
      "‰πî               -> O\n",
      "Ê≤ª               -> O\n",
      "Ë°®               -> O\n",
      "Á§∫               -> O\n",
      "‰∏≠               -> B-PLACE\n",
      "Âçé               -> I-PLACE\n",
      "‰∫∫               -> I-PLACE\n",
      "Ê∞ë               -> I-PLACE\n",
      "ÂÖ±               -> I-PLACE\n",
      "Âíå               -> I-PLACE\n",
      "ÂõΩ               -> I-PLACE\n",
      "ÁöÑ               -> O\n",
      "Ê≠º               -> O\n",
      "20              -> O\n",
      "Êàò               -> O\n",
      "Êú∫               -> O\n",
      "Âæà               -> O\n",
      "‰ºò               -> O\n",
      "ÁßÄ               -> O\n",
      "„ÄÇ               -> O\n",
      "[SEP]           -> O\n"
     ]
    }
   ],
   "source": [
    "# Ê®°ÂûãÈ¢ÑÊµã\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoding)\n",
    "\n",
    "# ÊèêÂèñÈ¢ÑÊµãÁªìÊûú\n",
    "logits = outputs.logits\n",
    "predicted_class_ids = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
    "\n",
    "# Â∞ÜÈ¢ÑÊµãÁªìÊûúÊò†Â∞Ñ‰∏∫Ê†áÁ≠æÔºåÂπ∂Â∞ÜÊ†áÁ≠æ‰∏éÂéüÂßãÊñáÊú¨ÂØπÂ∫îËµ∑Êù•\n",
    "predicted_labels = [id2label[class_id] for class_id in predicted_class_ids]\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"].squeeze().tolist())\n",
    "results = list(zip(tokens, predicted_labels))\n",
    "\n",
    "# ÊâìÂç∞È¢ÑÊµãÁªìÊûú\n",
    "print(\"ËæìÂÖ•ÊñáÊú¨:\", input_text)\n",
    "print(\"È¢ÑÊµãÁªìÊûú:\")\n",
    "for token, label in results:\n",
    "    print(f\"{token:15} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## È¢ÑÊµã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂÅáËÆæÈúÄË¶ÅÈ¢ÑÊµãÁöÑÊñáÊú¨Âú®‰∏Ä‰∏™ DataFrame ‰∏≠\n",
    "texts = [\n",
    "    \"Êó•Êú¨È¶ñÁõ∏ËèÖ‰πâ‰ºüËØ¥Êó•Êú¨Â∞ÜÁªßÁª≠‰∏é‰∏≠ÂõΩÂêà‰Ωú„ÄÇ\",\n",
    "    \"‰ªäÂ§©ÔºåÁæéÂà©ÂùöÂêà‰ºóÂõΩÂõΩÈò≤ÈÉ®ÂèëË®Ä‰∫∫‰πîÊ≤ªËØ¥‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊ≠º20ÊàòÊú∫Âæà‰ºòÁßÄ„ÄÇ\",\n",
    "    \"ÁæéÂõΩÊÄªÁªüÊãúÁôªËØ¥ÁæéÂõΩÂ∞ÜÁªßÁª≠ÊîØÊåÅ‰πåÂÖãÂÖ∞„ÄÇ\",\n",
    "    \"‰∏≠ÂõΩÂõΩÂÆ∂‰∏ªÂ∏≠‰π†ËøëÂπ≥ËØ¥‰∏≠ÂõΩÂ∞ÜÁªßÁª≠Êé®ËøõÂÖ®ÁêÉÂåñ„ÄÇ\",\n",
    "    \"‰øÑÁΩóÊñØÊÄªÁªüÊôÆ‰∫¨ËØ¥‰øÑÁΩóÊñØÂ∞ÜÁªßÁª≠ÊîØÊåÅÂèôÂà©‰∫ö„ÄÇ\",\n",
    "]\n",
    "df = pd.DataFrame(data={'text':texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Âä†ËΩΩÊ®°Âûã\n",
    "best_ckpt_path = 'model/checkpoint-78'  # Ëá™Â∑±ÂàáÊç¢‰∏∫ËÆ≠ÁªÉÂæóÂà∞ÁöÑÊúÄ‰Ω≥Ê®°ÂûãË∑ØÂæÑ\n",
    "model = AutoModelForTokenClassification.from_pretrained(best_ckpt_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(best_ckpt_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Áî®‰∫éÈ¢ÑÊµãÁöÑÂáΩÊï∞\n",
    "def predict(texts):\n",
    "    # ÂØπÊØè‰∏™ÊñáÊú¨ËøõË°åtokenization\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH)\n",
    "    \n",
    "    # Â∞ÜÊ®°ÂûãÊîæÂà∞ËØÑ‰º∞Ê®°Âºè\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Ëé∑ÂèñÊ®°ÂûãËæìÂá∫ (logits)\n",
    "        outputs = model(**encodings)\n",
    "        \n",
    "    # Ëé∑ÂèñÈ¢ÑÊµãÁªìÊûú\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    # Ê†πÊçÆÊúÄÂ§ßÊ¶ÇÁéáÈÄâÊã©È¢ÑÊµãÁöÑÊ†áÁ≠æ\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # ÁßªÈô§ [CLS] Âíå [SEP] token ÁöÑÈ¢ÑÊµãÁªìÊûú\n",
    "    # [CLS] ÊòØÁ¨¨‰∏Ä‰∏™tokenÔºå [SEP] ÊòØÊúÄÂêé‰∏Ä‰∏™token\n",
    "    # Âõ†Ê≠§Âú®ËøõË°åÈ¢ÑÊµãÂêéÔºåÈúÄË¶ÅÂéªÊéâÁ¨¨‰∏Ä‰∏™ÂíåÊúÄÂêé‰∏Ä‰∏™È¢ÑÊµã\n",
    "    input_ids = encodings['input_ids']\n",
    "    filtered_predictions = []\n",
    "    \n",
    "    for i, input_id in enumerate(input_ids):\n",
    "        # Ëé∑ÂèñÂΩìÂâçÂè•Â≠êÁöÑÂÆûÈôÖtokenÈÉ®ÂàÜÔºàÂéªÊéâ[CLS]Âíå[SEP]Ôºâ\n",
    "        valid_pred = predictions[i][1:-1]  # ÁßªÈô§Á¨¨‰∏Ä‰∏™ÂíåÊúÄÂêé‰∏Ä‰∏™È¢ÑÊµã\n",
    "        filtered_predictions.append(valid_pred)\n",
    "    \n",
    "    return filtered_predictions\n",
    "\n",
    "# ÊâßË°åÈ¢ÑÊµã\n",
    "df['predictions'] = predict(df['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Êó•Êú¨È¶ñÁõ∏ËèÖ‰πâ‰ºüËØ¥Êó•Êú¨Â∞ÜÁªßÁª≠‰∏é‰∏≠ÂõΩÂêà‰Ωú„ÄÇ\n",
      "Predicted Labels: [1, 2, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Text: ‰ªäÂ§©ÔºåÁæéÂà©ÂùöÂêà‰ºóÂõΩÂõΩÈò≤ÈÉ®ÂèëË®Ä‰∫∫‰πîÊ≤ªËØ¥‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊ≠º20ÊàòÊú∫Âæà‰ºòÁßÄ„ÄÇ\n",
      "Predicted Labels: [0, 0, 0, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Text: ÁæéÂõΩÊÄªÁªüÊãúÁôªËØ¥ÁæéÂõΩÂ∞ÜÁªßÁª≠ÊîØÊåÅ‰πåÂÖãÂÖ∞„ÄÇ\n",
      "Predicted Labels: [1, 2, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0]\n",
      "Text: ‰∏≠ÂõΩÂõΩÂÆ∂‰∏ªÂ∏≠‰π†ËøëÂπ≥ËØ¥‰∏≠ÂõΩÂ∞ÜÁªßÁª≠Êé®ËøõÂÖ®ÁêÉÂåñ„ÄÇ\n",
      "Predicted Labels: [1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Text: ‰øÑÁΩóÊñØÊÄªÁªüÊôÆ‰∫¨ËØ¥‰øÑÁΩóÊñØÂ∞ÜÁªßÁª≠ÊîØÊåÅÂèôÂà©‰∫ö„ÄÇ\n",
      "Predicted Labels: [1, 2, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# ËæìÂá∫ÁªìÊûú (Ê†πÊçÆÊ®°ÂûãÊ†áÁ≠æÊï∞ÈáèÔºåÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Ëß£Á†Å‰∏∫Ê†áÁ≠æÂêçÁß∞)\n",
    "for i, text in enumerate(df['text']):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Predicted Labels: {df.loc[i,'predictions'].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Êó•Êú¨È¶ñÁõ∏ËèÖ‰πâ‰ºüËØ¥Êó•Êú¨Â∞ÜÁªßÁª≠‰∏é‰∏≠ÂõΩÂêà‰Ωú„ÄÇ\n",
      "Predicted Labels:\n",
      "Êó•               -> B-PLACE\n",
      "Êú¨               -> I-PLACE\n",
      "È¶ñ               -> O\n",
      "Áõ∏               -> O\n",
      "ËèÖ               -> O\n",
      "‰πâ               -> O\n",
      "‰ºü               -> O\n",
      "ËØ¥               -> O\n",
      "Êó•               -> B-PLACE\n",
      "Êú¨               -> I-PLACE\n",
      "Â∞Ü               -> O\n",
      "Áªß               -> O\n",
      "Áª≠               -> O\n",
      "‰∏é               -> O\n",
      "‰∏≠               -> B-PLACE\n",
      "ÂõΩ               -> I-PLACE\n",
      "Âêà               -> O\n",
      "‰Ωú               -> O\n",
      "„ÄÇ               -> O\n",
      "\n",
      "\n",
      "Text: ‰ªäÂ§©ÔºåÁæéÂà©ÂùöÂêà‰ºóÂõΩÂõΩÈò≤ÈÉ®ÂèëË®Ä‰∫∫‰πîÊ≤ªËØ¥‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊ≠º20ÊàòÊú∫Âæà‰ºòÁßÄ„ÄÇ\n",
      "Predicted Labels:\n",
      "‰ªä               -> O\n",
      "Â§©               -> O\n",
      "Ôºå               -> O\n",
      "Áæé               -> B-PLACE\n",
      "Âà©               -> I-PLACE\n",
      "Âùö               -> I-PLACE\n",
      "Âêà               -> I-PLACE\n",
      "‰ºó               -> I-PLACE\n",
      "ÂõΩ               -> I-PLACE\n",
      "ÂõΩ               -> O\n",
      "Èò≤               -> O\n",
      "ÈÉ®               -> O\n",
      "Âèë               -> O\n",
      "Ë®Ä               -> O\n",
      "‰∫∫               -> O\n",
      "‰πî               -> O\n",
      "Ê≤ª               -> O\n",
      "ËØ¥               -> O\n",
      "‰∏≠               -> B-PLACE\n",
      "Âçé               -> I-PLACE\n",
      "‰∫∫               -> I-PLACE\n",
      "Ê∞ë               -> I-PLACE\n",
      "ÂÖ±               -> I-PLACE\n",
      "Âíå               -> I-PLACE\n",
      "ÂõΩ               -> I-PLACE\n",
      "ÁöÑ               -> O\n",
      "Ê≠º               -> O\n",
      "20              -> O\n",
      "Êàò               -> O\n",
      "Êú∫               -> O\n",
      "Âæà               -> O\n",
      "‰ºò               -> O\n",
      "ÁßÄ               -> O\n",
      "„ÄÇ               -> O\n",
      "\n",
      "\n",
      "Text: ÁæéÂõΩÊÄªÁªüÊãúÁôªËØ¥ÁæéÂõΩÂ∞ÜÁªßÁª≠ÊîØÊåÅ‰πåÂÖãÂÖ∞„ÄÇ\n",
      "Predicted Labels:\n",
      "Áæé               -> B-PLACE\n",
      "ÂõΩ               -> I-PLACE\n",
      "ÊÄª               -> O\n",
      "Áªü               -> O\n",
      "Êãú               -> O\n",
      "Áôª               -> O\n",
      "ËØ¥               -> O\n",
      "Áæé               -> B-PLACE\n",
      "ÂõΩ               -> I-PLACE\n",
      "Â∞Ü               -> O\n",
      "Áªß               -> O\n",
      "Áª≠               -> O\n",
      "ÊîØ               -> O\n",
      "ÊåÅ               -> O\n",
      "‰πå               -> B-PLACE\n",
      "ÂÖã               -> I-PLACE\n",
      "ÂÖ∞               -> I-PLACE\n",
      "„ÄÇ               -> O\n",
      "\n",
      "\n",
      "Text: ‰∏≠ÂõΩÂõΩÂÆ∂‰∏ªÂ∏≠‰π†ËøëÂπ≥ËØ¥‰∏≠ÂõΩÂ∞ÜÁªßÁª≠Êé®ËøõÂÖ®ÁêÉÂåñ„ÄÇ\n",
      "Predicted Labels:\n",
      "‰∏≠               -> B-PLACE\n",
      "ÂõΩ               -> I-PLACE\n",
      "ÂõΩ               -> O\n",
      "ÂÆ∂               -> O\n",
      "‰∏ª               -> O\n",
      "Â∏≠               -> O\n",
      "‰π†               -> O\n",
      "Ëøë               -> O\n",
      "Âπ≥               -> O\n",
      "ËØ¥               -> O\n",
      "‰∏≠               -> B-PLACE\n",
      "ÂõΩ               -> I-PLACE\n",
      "Â∞Ü               -> O\n",
      "Áªß               -> O\n",
      "Áª≠               -> O\n",
      "Êé®               -> O\n",
      "Ëøõ               -> O\n",
      "ÂÖ®               -> O\n",
      "ÁêÉ               -> O\n",
      "Âåñ               -> O\n",
      "„ÄÇ               -> O\n",
      "\n",
      "\n",
      "Text: ‰øÑÁΩóÊñØÊÄªÁªüÊôÆ‰∫¨ËØ¥‰øÑÁΩóÊñØÂ∞ÜÁªßÁª≠ÊîØÊåÅÂèôÂà©‰∫ö„ÄÇ\n",
      "Predicted Labels:\n",
      "‰øÑ               -> B-PLACE\n",
      "ÁΩó               -> I-PLACE\n",
      "ÊñØ               -> I-PLACE\n",
      "ÊÄª               -> O\n",
      "Áªü               -> O\n",
      "ÊôÆ               -> O\n",
      "‰∫¨               -> O\n",
      "ËØ¥               -> O\n",
      "‰øÑ               -> B-PLACE\n",
      "ÁΩó               -> I-PLACE\n",
      "ÊñØ               -> I-PLACE\n",
      "Â∞Ü               -> O\n",
      "Áªß               -> O\n",
      "Áª≠               -> O\n",
      "ÊîØ               -> O\n",
      "ÊåÅ               -> O\n",
      "Âèô               -> B-PLACE\n",
      "Âà©               -> I-PLACE\n",
      "‰∫ö               -> I-PLACE\n",
      "„ÄÇ               -> O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    0: 'O',\n",
    "    1: 'B-PLACE',\n",
    "    2: 'I-PLACE'\n",
    "}\n",
    "\n",
    "# ÊâìÂç∞È¢ÑÊµãÁªìÊûú\n",
    "for i, text in enumerate(df['text']):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Predicted Labels:\")\n",
    "    for token, label_id in zip(tokenizer.tokenize(text), df.loc[i,'predictions']):\n",
    "        label = id2label[label_id.item()]\n",
    "        print(f\"{token:15} -> {label}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>predictions</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Êó•Êú¨È¶ñÁõ∏ËèÖ‰πâ‰ºüËØ¥Êó•Êú¨Â∞ÜÁªßÁª≠‰∏é‰∏≠ÂõΩÂêà‰Ωú„ÄÇ</td>\n",
       "      <td>[tensor(1), tensor(2), tensor(0), tensor(0), t...</td>\n",
       "      <td>[B-PLACE, I-PLACE, O, O, O, O, O, O, B-PLACE, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‰ªäÂ§©ÔºåÁæéÂà©ÂùöÂêà‰ºóÂõΩÂõΩÈò≤ÈÉ®ÂèëË®Ä‰∫∫‰πîÊ≤ªËØ¥‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊ≠º20ÊàòÊú∫Âæà‰ºòÁßÄ„ÄÇ</td>\n",
       "      <td>[tensor(0), tensor(0), tensor(0), tensor(1), t...</td>\n",
       "      <td>[O, O, O, B-PLACE, I-PLACE, I-PLACE, I-PLACE, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ÁæéÂõΩÊÄªÁªüÊãúÁôªËØ¥ÁæéÂõΩÂ∞ÜÁªßÁª≠ÊîØÊåÅ‰πåÂÖãÂÖ∞„ÄÇ</td>\n",
       "      <td>[tensor(1), tensor(2), tensor(0), tensor(0), t...</td>\n",
       "      <td>[B-PLACE, I-PLACE, O, O, O, O, O, B-PLACE, I-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‰∏≠ÂõΩÂõΩÂÆ∂‰∏ªÂ∏≠‰π†ËøëÂπ≥ËØ¥‰∏≠ÂõΩÂ∞ÜÁªßÁª≠Êé®ËøõÂÖ®ÁêÉÂåñ„ÄÇ</td>\n",
       "      <td>[tensor(1), tensor(2), tensor(0), tensor(0), t...</td>\n",
       "      <td>[B-PLACE, I-PLACE, O, O, O, O, O, O, O, O, B-P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‰øÑÁΩóÊñØÊÄªÁªüÊôÆ‰∫¨ËØ¥‰øÑÁΩóÊñØÂ∞ÜÁªßÁª≠ÊîØÊåÅÂèôÂà©‰∫ö„ÄÇ</td>\n",
       "      <td>[tensor(1), tensor(2), tensor(2), tensor(0), t...</td>\n",
       "      <td>[B-PLACE, I-PLACE, I-PLACE, O, O, O, O, O, B-P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text  \\\n",
       "0                  Êó•Êú¨È¶ñÁõ∏ËèÖ‰πâ‰ºüËØ¥Êó•Êú¨Â∞ÜÁªßÁª≠‰∏é‰∏≠ÂõΩÂêà‰Ωú„ÄÇ   \n",
       "1  ‰ªäÂ§©ÔºåÁæéÂà©ÂùöÂêà‰ºóÂõΩÂõΩÈò≤ÈÉ®ÂèëË®Ä‰∫∫‰πîÊ≤ªËØ¥‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊ≠º20ÊàòÊú∫Âæà‰ºòÁßÄ„ÄÇ   \n",
       "2                   ÁæéÂõΩÊÄªÁªüÊãúÁôªËØ¥ÁæéÂõΩÂ∞ÜÁªßÁª≠ÊîØÊåÅ‰πåÂÖãÂÖ∞„ÄÇ   \n",
       "3                ‰∏≠ÂõΩÂõΩÂÆ∂‰∏ªÂ∏≠‰π†ËøëÂπ≥ËØ¥‰∏≠ÂõΩÂ∞ÜÁªßÁª≠Êé®ËøõÂÖ®ÁêÉÂåñ„ÄÇ   \n",
       "4                 ‰øÑÁΩóÊñØÊÄªÁªüÊôÆ‰∫¨ËØ¥‰øÑÁΩóÊñØÂ∞ÜÁªßÁª≠ÊîØÊåÅÂèôÂà©‰∫ö„ÄÇ   \n",
       "\n",
       "                                         predictions  \\\n",
       "0  [tensor(1), tensor(2), tensor(0), tensor(0), t...   \n",
       "1  [tensor(0), tensor(0), tensor(0), tensor(1), t...   \n",
       "2  [tensor(1), tensor(2), tensor(0), tensor(0), t...   \n",
       "3  [tensor(1), tensor(2), tensor(0), tensor(0), t...   \n",
       "4  [tensor(1), tensor(2), tensor(2), tensor(0), t...   \n",
       "\n",
       "                                              labels  \n",
       "0  [B-PLACE, I-PLACE, O, O, O, O, O, O, B-PLACE, ...  \n",
       "1  [O, O, O, B-PLACE, I-PLACE, I-PLACE, I-PLACE, ...  \n",
       "2  [B-PLACE, I-PLACE, O, O, O, O, O, B-PLACE, I-P...  \n",
       "3  [B-PLACE, I-PLACE, O, O, O, O, O, O, O, O, B-P...  \n",
       "4  [B-PLACE, I-PLACE, I-PLACE, O, O, O, O, O, B-P...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ËΩ¨Êç¢df['predictions']‰∏∫Ê†áÁ≠æ\n",
    "df['labels'] = df['predictions'].apply(lambda x: [id2label[i.item()] for i in x])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‰ΩøÁî® pipeline È¢ÑÊµã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(best_ckpt_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_ckpt_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'LABEL_0', 'score': 0.97941077, 'index': 1, 'word': '‰ªä', 'start': 0, 'end': 1}, {'entity': 'LABEL_0', 'score': 0.99748343, 'index': 2, 'word': 'Â§©', 'start': 1, 'end': 2}, {'entity': 'LABEL_0', 'score': 0.99748224, 'index': 3, 'word': 'Ôºå', 'start': 2, 'end': 3}, {'entity': 'LABEL_1', 'score': 0.9746602, 'index': 4, 'word': 'Áæé', 'start': 3, 'end': 4}, {'entity': 'LABEL_2', 'score': 0.9916592, 'index': 5, 'word': 'Âà©', 'start': 4, 'end': 5}, {'entity': 'LABEL_2', 'score': 0.9965677, 'index': 6, 'word': 'Âùö', 'start': 5, 'end': 6}, {'entity': 'LABEL_2', 'score': 0.906812, 'index': 7, 'word': 'Âêà', 'start': 6, 'end': 7}, {'entity': 'LABEL_2', 'score': 0.993753, 'index': 8, 'word': '‰ºó', 'start': 7, 'end': 8}, {'entity': 'LABEL_2', 'score': 0.9981445, 'index': 9, 'word': 'ÂõΩ', 'start': 8, 'end': 9}, {'entity': 'LABEL_0', 'score': 0.9979563, 'index': 10, 'word': 'ÂõΩ', 'start': 9, 'end': 10}, {'entity': 'LABEL_0', 'score': 0.9983614, 'index': 11, 'word': 'Èò≤', 'start': 10, 'end': 11}, {'entity': 'LABEL_0', 'score': 0.9994324, 'index': 12, 'word': 'ÈÉ®', 'start': 11, 'end': 12}, {'entity': 'LABEL_0', 'score': 0.9995034, 'index': 13, 'word': 'Âèë', 'start': 12, 'end': 13}, {'entity': 'LABEL_0', 'score': 0.9996099, 'index': 14, 'word': 'Ë®Ä', 'start': 13, 'end': 14}, {'entity': 'LABEL_0', 'score': 0.99967897, 'index': 15, 'word': '‰∫∫', 'start': 14, 'end': 15}, {'entity': 'LABEL_0', 'score': 0.998808, 'index': 16, 'word': '‰πî', 'start': 15, 'end': 16}, {'entity': 'LABEL_0', 'score': 0.99887043, 'index': 17, 'word': 'Ê≤ª', 'start': 16, 'end': 17}, {'entity': 'LABEL_0', 'score': 0.9993299, 'index': 18, 'word': 'Ë°®', 'start': 17, 'end': 18}, {'entity': 'LABEL_0', 'score': 0.99920684, 'index': 19, 'word': 'Á§∫', 'start': 18, 'end': 19}, {'entity': 'LABEL_1', 'score': 0.99556524, 'index': 20, 'word': '‰∏≠', 'start': 19, 'end': 20}, {'entity': 'LABEL_2', 'score': 0.9886613, 'index': 21, 'word': 'Âçé', 'start': 20, 'end': 21}, {'entity': 'LABEL_2', 'score': 0.9964683, 'index': 22, 'word': '‰∫∫', 'start': 21, 'end': 22}, {'entity': 'LABEL_2', 'score': 0.9975259, 'index': 23, 'word': 'Ê∞ë', 'start': 22, 'end': 23}, {'entity': 'LABEL_2', 'score': 0.997586, 'index': 24, 'word': 'ÂÖ±', 'start': 23, 'end': 24}, {'entity': 'LABEL_2', 'score': 0.99785006, 'index': 25, 'word': 'Âíå', 'start': 24, 'end': 25}, {'entity': 'LABEL_2', 'score': 0.99873155, 'index': 26, 'word': 'ÂõΩ', 'start': 25, 'end': 26}, {'entity': 'LABEL_0', 'score': 0.9933843, 'index': 27, 'word': 'ÁöÑ', 'start': 26, 'end': 27}, {'entity': 'LABEL_0', 'score': 0.9960901, 'index': 28, 'word': 'Ê≠º', 'start': 27, 'end': 28}, {'entity': 'LABEL_0', 'score': 0.98626316, 'index': 29, 'word': '20', 'start': 28, 'end': 30}, {'entity': 'LABEL_0', 'score': 0.9965487, 'index': 30, 'word': 'Êàò', 'start': 30, 'end': 31}, {'entity': 'LABEL_0', 'score': 0.9967501, 'index': 31, 'word': 'Êú∫', 'start': 31, 'end': 32}, {'entity': 'LABEL_0', 'score': 0.9971187, 'index': 32, 'word': 'Âæà', 'start': 32, 'end': 33}, {'entity': 'LABEL_0', 'score': 0.9971826, 'index': 33, 'word': '‰ºò', 'start': 33, 'end': 34}, {'entity': 'LABEL_0', 'score': 0.9963052, 'index': 34, 'word': 'ÁßÄ', 'start': 34, 'end': 35}, {'entity': 'LABEL_0', 'score': 0.9990269, 'index': 35, 'word': '„ÄÇ', 'start': 35, 'end': 36}]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer, device=device)\n",
    "text = \"‰ªäÂ§©ÔºåÁæéÂà©ÂùöÂêà‰ºóÂõΩÂõΩÈò≤ÈÉ®ÂèëË®Ä‰∫∫‰πîÊ≤ªË°®Á§∫‰∏≠Âçé‰∫∫Ê∞ëÂÖ±ÂíåÂõΩÁöÑÊ≠º20ÊàòÊú∫Âæà‰ºòÁßÄ„ÄÇ\"\n",
    "ner_results = nlp(text)\n",
    "print(ner_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
